{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4T_klvvlUPu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a242f51-04eb-4cec-ee98-2999002782cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Check GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUPLnM7mIt0v",
        "outputId": "7eaedc37-45fa-4222-fd5c-4acaa8c3728d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzhXaGbilUqX",
        "outputId": "72fade71-5509-4077-e3a2-4f4d388f416e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       abr                                      long abr_text  start_abr  \\\n",
            "0       AB                                  abortion      AB.        231   \n",
            "1       AB                                  abortion      AB.        249   \n",
            "2       AB                                  abortion       AB        223   \n",
            "3       AB                                  abortion      AB.        194   \n",
            "4       AB                                  abortion       AB        114   \n",
            "...    ...                                       ...      ...        ...   \n",
            "37495  VAD  vincristine adriamycin and dexamethasone     VAD.        139   \n",
            "37496  VAD  vincristine adriamycin and dexamethasone     VAD,        172   \n",
            "37497  VAD  vincristine adriamycin and dexamethasone      VAD        250   \n",
            "37498  VAD  vincristine adriamycin and dexamethasone      VAD        181   \n",
            "37499  VAD  vincristine adriamycin and dexamethasone      VAD         60   \n",
            "\n",
            "       end_abr                         section  \\\n",
            "0          233                                   \n",
            "1          251                                   \n",
            "2          224                 PAST OB HISTORY   \n",
            "3          196  HISTORY OF THE PRESENT ILLNESS   \n",
            "4          115             PAST OB-GYN HISTORY   \n",
            "...        ...                             ...   \n",
            "37495      142            PAST MEDICAL HISTORY   \n",
            "37496      175                      IMPRESSION   \n",
            "37497      252      HISTORY OF PRESENT ILLNESS   \n",
            "37498      183      HISTORY OF PRESENT ILLNESS   \n",
            "37499       62                      PROCEDURES   \n",
            "\n",
            "                                                    text  \n",
            "0      _%#NAME#%_ _%#NAME#%_ is a 29-year-old gravida...  \n",
            "1      She is now bleeding quite heavily. Ultrasound ...  \n",
            "2      ALLERGIES: Heparin and Imitrex. PAST OB HISTOR...  \n",
            "3      She had a pelvic ultrasound at Park Nicollet o...  \n",
            "4      On _%#MMDD2007#%_, normal anatomy with anterio...  \n",
            "...                                                  ...  \n",
            "37495  1. Multiple myeloma, undergoing chemotherapy. ...  \n",
            "37496  He has been receiving weekly Procrit injection...  \n",
            "37497  Within a month, he developed recurrent hip pai...  \n",
            "37498  He had a serum protein electrophoresis with a ...  \n",
            "37499  DISCHARGE DIAGNOSES: Multiple myeloma. PROCEDU...  \n",
            "\n",
            "[37500 rows x 7 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Initialize an empty list to store data for the DataFrame\n",
        "data = []\n",
        "\n",
        "# Define the path to your text file\n",
        "file_path = '/content/drive/MyDrive/Abreviation/CASI/CASI.txt'\n",
        "\n",
        "# Read the file line by line\n",
        "with open(file_path, 'r',encoding='utf-8', errors='ignore') as file:\n",
        "  for line in file:\n",
        "    # Split the line by the '|' delimiter\n",
        "    parts = line.split('|')\n",
        "\n",
        "    # Extract elements based on the expected format\n",
        "    if len(parts) >= 7:  # Ensure the line has the expected number of fields\n",
        "        abr = parts[0]\n",
        "        long_form = parts[1]\n",
        "        abr_text = parts[2]\n",
        "        start_abr = int(parts[3])\n",
        "        end_abr = int(parts[4])\n",
        "        section = parts[5]  # May be empty if no section is specified\n",
        "        text = parts[6]\n",
        "\n",
        "        # Append the extracted fields to the data list as a dictionary\n",
        "        data.append({\n",
        "            'abr': abr,\n",
        "            'long': long_form,\n",
        "            'abr_text': abr_text,\n",
        "            'start_abr': start_abr,\n",
        "            'end_abr': end_abr,\n",
        "            'section': section,\n",
        "            'text': text\n",
        "        })\n",
        "\n",
        "# Convert the list of dictionaries to a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Display the resulting DataFrame\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iL_SAW1ElUqZ"
      },
      "outputs": [],
      "source": [
        "dff=pd.DataFrame(df, columns=('abr','long','text'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOJgLZjzI3e5",
        "outputId": "7478125d-0912-445e-f34d-6503e5600f27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max length of 'TEXT': 2708\n",
            "Maxn length of 'LABEL': 60\n",
            "Maxn length of 'LABEL': 4\n"
          ]
        }
      ],
      "source": [
        "# Calculate the mean length of the 'TEXT' column (number of characters)\n",
        "max_text_length = dff['text'].apply(len).max()\n",
        "\n",
        "# Calculate the mean length of the 'LABEL' column (number of characters)\n",
        "max_label_length =dff['long'].apply(len).max()\n",
        "# Calculate the mean length of the 'abr' column (number of characters)\n",
        "max_abr_length =dff['abr'].apply(len).max()\n",
        "# Print the results\n",
        "print(f\"Max length of 'TEXT': {max_text_length}\")\n",
        "print(f\"Maxn length of 'LABEL': {max_label_length}\")\n",
        "print(f\"Maxn length of 'LABEL': {max_abr_length}\")\n",
        "from transformers import BartTokenizer\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEr7W1wNJ00D",
        "outputId": "801f0e25-28c3-4c60-9a91-8cf7d8838461"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Maximum token length in `abr`: 3\n",
            "Maximum token length in `text`: 899\n",
            "Maximum token length in `long`: 20\n"
          ]
        }
      ],
      "source": [
        "'''# Load the tokenizer\n",
        "from transformers import BartTokenizer\n",
        "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")'''\n",
        "\n",
        "# Calculate max token length for the `abr` column\n",
        "max_abr_token_length = dff['abr'].apply(lambda x: len(tokenizer.tokenize(x))).max()\n",
        "print(f\"Maximum token length in `abr`: {max_abr_token_length}\")\n",
        "# Calculate max token length for the `text` column\n",
        "max_text_token_length = dff['text'].apply(lambda x: len(tokenizer.tokenize(x))).max()\n",
        "print(f\"Maximum token length in `text`: {max_text_token_length}\")\n",
        "# Calculate max token length for the `long` column\n",
        "max_long_token_length = dff['long'].apply(lambda x: len(tokenizer.tokenize(x))).max()\n",
        "print(f\"Maximum token length in `long`: {max_long_token_length}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1_4DDZxlUqZ",
        "outputId": "a4470ed4-7dc6-4891-8077-27d290963635"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(37500, 3)\n"
          ]
        }
      ],
      "source": [
        "'''dff = df1.sample(n=15, random_state=42)  # `random_state` ensures reproducibility'''\n",
        "\n",
        "dff=df1\n",
        "print(dff.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bp3UNfxKZJa2",
        "outputId": "5d9cbced-46fc-4e51-98d3-73b3fadc9612"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
        "\n",
        "# Load model and move to device\n",
        "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\").to(device)\n",
        "\n"
      ],
      "metadata": {
        "id": "ruU0UxZqZLe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321,
          "referenced_widgets": [
            "e624d47635b24d7fbc4bfee8e47d0203",
            "0b17152d3dd24809a4abaecbce794546",
            "7be038234a3f4ac1813769e50610b7e8",
            "53658098b00943778cd9f06f9b17e01d",
            "3fa6bf7687df4548a439b76f4de03ad1",
            "3b3b6ea2745947b2927ed7156c65b208",
            "e91a0784e2be4b88bf56b5e55c94119b",
            "b5db216ff8f44b9f9c005f74cd33bf73",
            "89a357c98f29451e9d33750e4701fbe3",
            "cd7a1e44d5a84f37bfa0acb718d371af",
            "6d799d5343b04ba4ae41628dc0e57c87",
            "52e7b85655e5456a97eb8b45aa74da3b",
            "5da6f54e620b46b0ba82747e5023dd69",
            "4990b51eb3ea4d79a0719fffeb2979a5",
            "6b7305eecd5c408d84a0248524712550",
            "9497df3847df4bc28d47e9754da1ee89",
            "63a9755b9434451e8e7ac6eee5a22dc6",
            "9b0f0c0f685e4682a294a076d89b323e",
            "d0219075dd1a42a88592c943e50e0f19",
            "a3f6f5e9fed54b139dbfd1192539d3c3",
            "6264e882ab9a4048a04f8ca471a30b71",
            "f858237bad74476488c2a3204565329b",
            "d7de0ab94470430794a5a46441d55f17",
            "e4e886c2a4da41a1b0ce36839f0028a2",
            "f73318460fd94e0b8a069ecb5818e5bf",
            "03b68632d71f4b78ba6ee4d4095b4fd2",
            "b9f40c42baf24412ad1c257aa1187261",
            "ef899ccc98284be08e03f2b2bca195b2",
            "d5cff1077ebe43f29c5b624a56d3cdce",
            "56e333a774784208b896326dfcd59f3c",
            "c8b5e016891a434ead38359553938745",
            "c43325e30ff347458996e6f9f109919d",
            "18d025c632ca4741acc16f08d4bb73a5",
            "d2037dd0669b45839c1bd14ddc6ffc05",
            "199ffff7d4c543448cb1661b6596d1dc",
            "07895e0dde394369a31e76e6c8f15b52",
            "694fc5bf4c41452aa17d318357d2cc50",
            "dd7644b6b4d74e118b4468c44b6442bc",
            "98fea2c6e34c4ba8916f3f1d3b9106ae",
            "d943537168dc4f559b4f297c0d2d2904",
            "94d34072a6794bfda260a85ca0c4c38a",
            "f0e518f21a664db5ae48b1594a11cd2f",
            "58d0b6ce41fb4029889fa133c658de39",
            "abdb9ce65d5743baa4be687bc2aa67e7"
          ]
        },
        "id": "Yw9zABfE89sT",
        "outputId": "591c115f-d7cb-4344-e555-6ec194f2b14c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e624d47635b24d7fbc4bfee8e47d0203"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "52e7b85655e5456a97eb8b45aa74da3b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d7de0ab94470430794a5a46441d55f17"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.72k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d2037dd0669b45839c1bd14ddc6ffc05"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(28125, 3)\n",
            "(7031, 3)\n",
            "(2344, 3)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BartTokenizer\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "# Tokenizer\n",
        "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
        "def split_into_chunks(text, tokenizer, chunk_size=512):\n",
        "    # Ensure the input is a string, not a list\n",
        "    if isinstance(text, list):\n",
        "        text = \" \".join(text)  # Join the list elements into a single string\n",
        "\n",
        "    tokens = tokenizer.tokenize(text)  # Tokenize the text\n",
        "    return [\" \".join(tokens[i:i + chunk_size]) for i in range(0, len(tokens), chunk_size)]\n",
        "\n",
        "class AbbreviationDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_length=512, chunk_size=512):\n",
        "        self.dataframe = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.chunk_size = chunk_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.dataframe.iloc[idx]\n",
        "        input_text = f\"{row['abr']} - {row['text']}\"\n",
        "        target_text = row[\"long\"]\n",
        "\n",
        "        # Split the input text into chunks\n",
        "        chunks = split_into_chunks(input_text, self.tokenizer, chunk_size=self.chunk_size)\n",
        "\n",
        "        # Tokenize input chunks and target text\n",
        "        inputs = self.tokenizer(\n",
        "            \" \".join(chunks),  # Join the chunks back into a string\n",
        "            max_length=self.max_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        targets = self.tokenizer(\n",
        "            target_text,\n",
        "            max_length=self.max_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": inputs[\"input_ids\"].squeeze(0).to(device),\n",
        "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0).to(device),\n",
        "            \"labels\": targets[\"input_ids\"].squeeze(0).to(device),\n",
        "        }\n",
        "\n",
        "# Split DataFrame into train, validation, and test sets\n",
        "train_df = dff.sample(frac=0.75, random_state=42)\n",
        "v_df = dff.drop(train_df.index)\n",
        "test_df = v_df.sample(frac=0.75, random_state=42)\n",
        "valid_df = v_df.drop(test_df.index)\n",
        "print(train_df.shape)\n",
        "print(test_df.shape)\n",
        "print(valid_df.shape)\n",
        "# Create datasets\n",
        "train_dataset = AbbreviationDataset(train_df, tokenizer)\n",
        "valid_dataset = AbbreviationDataset(valid_df,tokenizer)\n",
        "test_dataset = AbbreviationDataset(test_df, tokenizer)\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=8, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"#ok\n",
        "from transformers import BartForConditionalGeneration\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load model\n",
        "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\"\"\"\n",
        "model.to(device)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Training and validation loop\n",
        "epochs =\n",
        "\n",
        "def compute_metrics(preds, labels):\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    precision = precision_score(labels, preds, average='weighted', zero_division=0)\n",
        "    recall = recall_score(labels, preds, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(labels, preds, average='weighted', zero_division=0)\n",
        "    return accuracy, precision, recall, f1\n",
        "max_new_tokens=25\n",
        "for epoch in range(epochs):\n",
        "    # Training phase\n",
        "    print(epoch)\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    j=1\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        #print(j,\"  loss: \",loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        j=j+1\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {train_loss / len(train_loader):.4f}\")\n",
        "    model.save_pretrained(\"/content/drive/MyDrive/Abreviation/bart\")\n",
        "    tokenizer.save_pretrained(\"/content/drive/MyDrive/Abreviation/bart\")\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_predictions = []\n",
        "    val_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in valid_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            val_loss += outputs.loss.item()\n",
        "\n",
        "            # Decode predictions and labels for metric calculation\n",
        "            predictions = model.generate(input_ids, attention_mask=attention_mask)\n",
        "            val_predictions.extend([tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions])\n",
        "            val_labels.extend([tokenizer.decode(label, skip_special_tokens=True) for label in labels])\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {val_loss / len(valid_loader):.4f}\")\n",
        "    # Compute validation metrics\n",
        "    metrics = compute_metrics(val_predictions, val_labels)\n",
        "    print(f\"Validation Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0AGzjNI9LLm",
        "outputId": "f285a7ea-f9d7-46e7-f51d-306910205160"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "Epoch 1/4, Training Loss: 0.0027\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/4, Validation Loss: 0.0013\n",
            "Validation Metrics: Accuracy: 0.7982, Precision: 0.7908, Recall: 0.7982, F1: 0.7713\n",
            "1\n",
            "Epoch 2/4, Training Loss: 0.0012\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/4, Validation Loss: 0.0007\n",
            "Validation Metrics: Accuracy: 0.8468, Precision: 0.8636, Recall: 0.8468, F1: 0.8361\n",
            "2\n",
            "Epoch 3/4, Training Loss: 0.0009\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/4, Validation Loss: 0.0006\n",
            "Validation Metrics: Accuracy: 0.8784, Precision: 0.9080, Recall: 0.8784, F1: 0.8701\n",
            "3\n",
            "Epoch 4/4, Training Loss: 0.0005\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/4, Validation Loss: 0.0006\n",
            "Validation Metrics: Accuracy: 0.8793, Precision: 0.9098, Recall: 0.8793, F1: 0.8765\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PNEtwfx0o8Q"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"/content/drive/MyDrive/Abreviation/bart\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/Abreviation/bart\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_VRvH8YwZ_3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19d2087c-0b2d-4780-f685-331703be1b91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/4, test Loss: 0.0006\n",
            "Test Metrics: Accuracy: 0.8793, Precision: 0.9098, Recall: 0.8793, F1: 0.8765\n"
          ]
        }
      ],
      "source": [
        "# test phase\n",
        "\"\"\"test_dataset = AbbreviationDataset(test_df, tokenizer)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8)\"\"\"\n",
        "model.eval()\n",
        "test_loss = 0\n",
        "test_predictions = []\n",
        "test_labels = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        test_loss += outputs.loss.item()\n",
        "\n",
        "        # Decode predictions and labels for metric calculation\n",
        "        predictions = model.generate(input_ids, attention_mask=attention_mask)\n",
        "        test_predictions.extend([tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions])\n",
        "        test_labels.extend([tokenizer.decode(label, skip_special_tokens=True) for label in labels])\n",
        "\n",
        "print(f\"Epoch {epoch + 1}/{epochs}, test Loss: {test_loss / len(test_loader):.4f}\")\n",
        "# Compute test metrics\n",
        "metrics = compute_metrics(test_predictions, test_labels)\n",
        "print(f\"Test Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train phase\n",
        "model.eval()\n",
        "train_loss = 0\n",
        "train_predictions = []\n",
        "train_labels = []\n",
        "with torch.no_grad():\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        train_loss += outputs.loss.item()\n",
        "\n",
        "        # Decode predictions and labels for metric calculation\n",
        "        predictions = model.generate(input_ids, attention_mask=attention_mask)\n",
        "        train_predictions.extend([tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions])\n",
        "        train_labels.extend([tokenizer.decode(label, skip_special_tokens=True) for label in labels])\n",
        "\n",
        "print(f\"Epoch {epoch + 1}/{epochs}, train Loss: {train_loss / len(train_loader):.4f}\")\n",
        "# Compute train metrics\n",
        "metrics = compute_metrics(train_predictions, train_labels)\n",
        "print(f\"train Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KE8IG5dg6aGE",
        "outputId": "090dc732-a5ae-4ca2-83d9-fa888b098a22"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/4, train Loss: 0.0003\n",
            "train Metrics: Accuracy: 0.9022, Precision: 0.9366, Recall: 0.9022, F1: 0.9042\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2dvl0Uyc6ZsM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0dXgVXL2eyou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BartForConditionalGeneration.from_pretrained(\"/content/drive/MyDrive/Abreviation/bart1\")\n",
        "# Load the tokenizer\n",
        "tokenizer = BartTokenizer.from_pretrained(\"/content/drive/MyDrive/Abreviation/bart1\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfakHblxezHY",
        "outputId": "6852fdf8-e88f-4624-b52a-5e94ca15d8ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BartForConditionalGeneration(\n",
              "  (model): BartModel(\n",
              "    (shared): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
              "    (encoder): BartEncoder(\n",
              "      (embed_tokens): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
              "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x BartEncoderLayer(\n",
              "          (self_attn): BartSdpaAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (decoder): BartDecoder(\n",
              "      (embed_tokens): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
              "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x BartDecoderLayer(\n",
              "          (self_attn): BartSdpaAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartSdpaAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f491da5-9412-4496-ad63-f68689ab52a0",
        "id": "cqzVzWgIezHa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/4, test Loss: 0.0036\n",
            "Test Metrics: Accuracy: 0.6271, Precision: 0.5356, Recall: 0.6271, F1: 0.5449\n"
          ]
        }
      ],
      "source": [
        "# test phase\n",
        "test_dataset = AbbreviationDataset(test_df, tokenizer)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8)\n",
        "model.eval()\n",
        "test_loss = 0\n",
        "test_predictions = []\n",
        "test_labels = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        test_loss += outputs.loss.item()\n",
        "\n",
        "        # Decode predictions and labels for metric calculation\n",
        "        predictions = model.generate(input_ids, attention_mask=attention_mask)\n",
        "        test_predictions.extend([tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions])\n",
        "        test_labels.extend([tokenizer.decode(label, skip_special_tokens=True) for label in labels])\n",
        "\n",
        "print(f\"Epoch {epoch + 1}/{epochs}, test Loss: {test_loss / len(test_loader):.4f}\")\n",
        "# Compute test metrics\n",
        "metrics = compute_metrics(test_predictions, test_labels)\n",
        "print(f\"Test Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train phase\n",
        "model.eval()\n",
        "train_loss = 0\n",
        "train_predictions = []\n",
        "train_labels = []\n",
        "with torch.no_grad():\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        train_loss += outputs.loss.item()\n",
        "\n",
        "        # Decode predictions and labels for metric calculation\n",
        "        predictions = model.generate(input_ids, attention_mask=attention_mask)\n",
        "        train_predictions.extend([tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions])\n",
        "        train_labels.extend([tokenizer.decode(label, skip_special_tokens=True) for label in labels])\n",
        "\n",
        "print(f\"Epoch {epoch + 1}/{epochs}, train Loss: {train_loss / len(train_loader):.4f}\")\n",
        "# Compute train metrics\n",
        "metrics = compute_metrics(train_predictions, train_labels)\n",
        "print(f\"train Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc8e8aac-49e8-487a-e854-330d58d92a1d",
        "id": "QiXdtwKnezHb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/4, train Loss: 0.0036\n",
            "train Metrics: Accuracy: 0.6185, Precision: 0.5502, Recall: 0.6185, F1: 0.5355\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a9WSt7Ftf0CD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BartForConditionalGeneration.from_pretrained(\"/content/drive/MyDrive/Abreviation/bart2\")\n",
        "# Load the tokenizer\n",
        "tokenizer = BartTokenizer.from_pretrained(\"/content/drive/MyDrive/Abreviation/bart2\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "625c4a02-a5f3-4041-e80d-bc934bdaf171",
        "id": "6ppfitAFf0f-"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BartForConditionalGeneration(\n",
              "  (model): BartModel(\n",
              "    (shared): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
              "    (encoder): BartEncoder(\n",
              "      (embed_tokens): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
              "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x BartEncoderLayer(\n",
              "          (self_attn): BartSdpaAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (decoder): BartDecoder(\n",
              "      (embed_tokens): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
              "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x BartDecoderLayer(\n",
              "          (self_attn): BartSdpaAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartSdpaAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa0e5e55-9a1d-4816-f84e-24045cf6234b",
        "id": "JlMKgabKf0f_"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/4, test Loss: 0.0011\n",
            "Test Metrics: Accuracy: 0.7987, Precision: 0.8039, Recall: 0.7987, F1: 0.7728\n"
          ]
        }
      ],
      "source": [
        "# test phase\n",
        "\"\"\"test_dataset = AbbreviationDataset(test_df, tokenizer)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8)\"\"\"\n",
        "model.eval()\n",
        "test_loss = 0\n",
        "test_predictions = []\n",
        "test_labels = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        test_loss += outputs.loss.item()\n",
        "\n",
        "        # Decode predictions and labels for metric calculation\n",
        "        predictions = model.generate(input_ids, attention_mask=attention_mask)\n",
        "        test_predictions.extend([tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions])\n",
        "        test_labels.extend([tokenizer.decode(label, skip_special_tokens=True) for label in labels])\n",
        "\n",
        "print(f\"Epoch {epoch + 1}/{epochs}, test Loss: {test_loss / len(test_loader):.4f}\")\n",
        "# Compute test metrics\n",
        "metrics = compute_metrics(test_predictions, test_labels)\n",
        "print(f\"Test Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train phase\n",
        "model.eval()\n",
        "train_loss = 0\n",
        "train_predictions = []\n",
        "train_labels = []\n",
        "with torch.no_grad():\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        train_loss += outputs.loss.item()\n",
        "\n",
        "        # Decode predictions and labels for metric calculation\n",
        "        predictions = model.generate(input_ids, attention_mask=attention_mask)\n",
        "        train_predictions.extend([tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions])\n",
        "        train_labels.extend([tokenizer.decode(label, skip_special_tokens=True) for label in labels])\n",
        "\n",
        "print(f\"Epoch {epoch + 1}/{epochs}, train Loss: {train_loss / len(train_loader):.4f}\")\n",
        "# Compute train metrics\n",
        "metrics = compute_metrics(train_predictions, train_labels)\n",
        "print(f\"train Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AnidpLsf0f_",
        "outputId": "95055ed6-8386-4ea3-8672-710c2a62eb58"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/4, train Loss: 0.0010\n",
            "train Metrics: Accuracy: 0.8004, Precision: 0.8292, Recall: 0.8004, F1: 0.7764\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartForConditionalGeneration\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ],
      "metadata": {
        "id": "15dPLUjxf-Yg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BartForConditionalGeneration.from_pretrained(\"/content/drive/MyDrive/Abreviation/bart3\")\n",
        "# Load the tokenizer\n",
        "tokenizer = BartTokenizer.from_pretrained(\"/content/drive/MyDrive/Abreviation/bart3\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "ioEKbn3Tf-zL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4f30953-78ce-41d1-f1bf-ed551505e117"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BartForConditionalGeneration(\n",
              "  (model): BartModel(\n",
              "    (shared): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
              "    (encoder): BartEncoder(\n",
              "      (embed_tokens): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
              "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x BartEncoderLayer(\n",
              "          (self_attn): BartSdpaAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (decoder): BartDecoder(\n",
              "      (embed_tokens): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
              "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x BartDecoderLayer(\n",
              "          (self_attn): BartSdpaAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartSdpaAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFMM_Z6Mf-zM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "8f781427-c5ca-4952-f58e-c29eddb80de1"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'epochs' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-bd0c03eb2115>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mtest_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch + 1}/{epochs}, test Loss: {test_loss / len(test_loader):.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;31m# Compute test metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'epochs' is not defined"
          ]
        }
      ],
      "source": [
        "# test phase\n",
        "\"\"\"test_dataset = AbbreviationDataset(test_df, tokenizer)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8)\"\"\"\n",
        "model.eval()\n",
        "test_loss = 0\n",
        "test_predictions = []\n",
        "test_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        test_loss += outputs.loss.item()\n",
        "\n",
        "        # Decode predictions and labels for metric calculation\n",
        "        predictions = model.generate(input_ids, attention_mask=attention_mask)\n",
        "        test_predictions.extend([tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions])\n",
        "        test_labels.extend([tokenizer.decode(label, skip_special_tokens=True) for label in labels])\n",
        "\n",
        "print(f\"Epoch {epoch + 1}/{epochs}, test Loss: {test_loss / len(test_loader):.4f}\")\n",
        "# Compute test metrics\n",
        "metrics = compute_metrics(test_predictions, test_labels)\n",
        "print(f\"Test Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(preds, labels):\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    precision = precision_score(labels, preds, average='weighted', zero_division=0)\n",
        "    recall = recall_score(labels, preds, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(labels, preds, average='weighted', zero_division=0)\n",
        "    return accuracy, precision, recall, f1\n",
        "epochs=5\n",
        "print(f\"Epoch {epoch + 1}/{epochs}, test Loss: {test_loss / len(test_loader):.4f}\")\n",
        "# Compute test metrics\n",
        "metrics = compute_metrics(test_predictions, test_labels)\n",
        "print(f\"Test Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbTxael1NnlZ",
        "outputId": "3aeffa71-d50d-41fd-b0a8-10c961802faa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5, test Loss: 0.0007\n",
            "Test Metrics: Accuracy: 0.8500, Precision: 0.8797, Recall: 0.8500, F1: 0.8426\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train phase\n",
        "model.eval()\n",
        "train_loss = 0\n",
        "train_predictions = []\n",
        "train_labels = []\n",
        "with torch.no_grad():\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        train_loss += outputs.loss.item()\n",
        "\n",
        "        # Decode predictions and labels for metric calculation\n",
        "        predictions = model.generate(input_ids, attention_mask=attention_mask)\n",
        "        train_predictions.extend([tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions])\n",
        "        train_labels.extend([tokenizer.decode(label, skip_special_tokens=True) for label in labels])\n",
        "\n",
        "print(f\"Epoch {epoch + 1}/{epochs}, train Loss: {train_loss / len(train_loader):.4f}\")\n",
        "# Compute train metrics\n",
        "metrics = compute_metrics(train_predictions, train_labels)\n",
        "print(f\"train Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n"
      ],
      "metadata": {
        "id": "w8-nNNVAf-zM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc7c7580-840a-4395-bd0b-e3fee85284ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5, train Loss: 0.0005\n",
            "train Metrics: Accuracy: 0.8615, Precision: 0.8890, Recall: 0.8615, F1: 0.8554\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HJ0JYc6bgE84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BartForConditionalGeneration.from_pretrained(\"/content/drive/MyDrive/Abreviation/bart4\")\n",
        "# Load the tokenizer\n",
        "tokenizer = BartTokenizer.from_pretrained(\"/content/drive/MyDrive/Abreviation/bart4\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "fvU9MetfgFgZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5c57272-33ab-43dd-c2b1-0cf01fd4126c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BartForConditionalGeneration(\n",
              "  (model): BartModel(\n",
              "    (shared): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
              "    (encoder): BartEncoder(\n",
              "      (embed_tokens): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
              "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x BartEncoderLayer(\n",
              "          (self_attn): BartSdpaAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (decoder): BartDecoder(\n",
              "      (embed_tokens): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
              "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x BartDecoderLayer(\n",
              "          (self_attn): BartSdpaAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartSdpaAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DF2ognFqgFga",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "547cfc5f-99f1-4210-b676-d14c612e2161"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5, test Loss: 0.0006\n",
            "Test Metrics: Accuracy: 0.8810, Precision: 0.8959, Recall: 0.8810, F1: 0.8765\n"
          ]
        }
      ],
      "source": [
        "# test phase\n",
        "\"\"\"test_dataset = AbbreviationDataset(test_df, tokenizer)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8)\"\"\"\n",
        "model.eval()\n",
        "test_loss = 0\n",
        "test_predictions = []\n",
        "test_labels = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        test_loss += outputs.loss.item()\n",
        "\n",
        "        # Decode predictions and labels for metric calculation\n",
        "        predictions = model.generate(input_ids, attention_mask=attention_mask)\n",
        "        test_predictions.extend([tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions])\n",
        "        test_labels.extend([tokenizer.decode(label, skip_special_tokens=True) for label in labels])\n",
        "\n",
        "print(f\"Epoch {epoch + 1}/{epochs}, test Loss: {test_loss / len(test_loader):.4f}\")\n",
        "# Compute test metrics\n",
        "metrics = compute_metrics(test_predictions, test_labels)\n",
        "print(f\"Test Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train phase\n",
        "model.eval()\n",
        "train_loss = 0\n",
        "train_predictions = []\n",
        "train_labels = []\n",
        "with torch.no_grad():\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        train_loss += outputs.loss.item()\n",
        "\n",
        "        # Decode predictions and labels for metric calculation\n",
        "        predictions = model.generate(input_ids, attention_mask=attention_mask)\n",
        "        train_predictions.extend([tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions])\n",
        "        train_labels.extend([tokenizer.decode(label, skip_special_tokens=True) for label in labels])\n",
        "\n",
        "print(f\"Epoch {epoch + 1}/{epochs}, train Loss: {train_loss / len(train_loader):.4f}\")\n",
        "# Compute train metrics\n",
        "metrics = compute_metrics(train_predictions, train_labels)\n",
        "print(f\"train Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n"
      ],
      "metadata": {
        "id": "1kBtw4B-gFga",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c97073e-3bec-4a0a-b230-b465ed672815"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5, train Loss: 0.0003\n",
            "train Metrics: Accuracy: 0.8921, Precision: 0.9223, Recall: 0.8921, F1: 0.8868\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jBfnlZrKoisI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BartForConditionalGeneration.from_pretrained(\"/content/drive/MyDrive/Abreviation/bart\")\n",
        "# Load the tokenizer\n",
        "tokenizer = BartTokenizer.from_pretrained(\"/content/drive/MyDrive/Abreviation/bart\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e73bdf9-6d1a-41f4-9f2c-3c4f9f88b6d9",
        "id": "nhwz5F6gojOD"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BartForConditionalGeneration(\n",
              "  (model): BartModel(\n",
              "    (shared): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
              "    (encoder): BartEncoder(\n",
              "      (embed_tokens): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
              "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x BartEncoderLayer(\n",
              "          (self_attn): BartSdpaAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (decoder): BartDecoder(\n",
              "      (embed_tokens): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
              "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x BartDecoderLayer(\n",
              "          (self_attn): BartSdpaAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartSdpaAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfc288c9-b455-45c1-e887-76c6a3dcb512",
        "id": "PjtMei8_ojOF"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5, test Loss: 0.0005\n",
            "Test Metrics: Accuracy: 0.8899, Precision: 0.9196, Recall: 0.8899, F1: 0.8921\n"
          ]
        }
      ],
      "source": [
        "# test phase\n",
        "\"\"\"test_dataset = AbbreviationDataset(test_df, tokenizer)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8)\"\"\"\n",
        "model.eval()\n",
        "test_loss = 0\n",
        "test_predictions = []\n",
        "test_labels = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        test_loss += outputs.loss.item()\n",
        "\n",
        "        # Decode predictions and labels for metric calculation\n",
        "        predictions = model.generate(input_ids, attention_mask=attention_mask)\n",
        "        test_predictions.extend([tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions])\n",
        "        test_labels.extend([tokenizer.decode(label, skip_special_tokens=True) for label in labels])\n",
        "\n",
        "print(f\"Epoch {epoch + 1}/{epochs}, test Loss: {test_loss / len(test_loader):.4f}\")\n",
        "# Compute test metrics\n",
        "metrics = compute_metrics(test_predictions, test_labels)\n",
        "print(f\"Test Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tzUYizxV5jzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BartForConditionalGeneration.from_pretrained(\"/content/drive/MyDrive/Abreviation/bart5\")\n",
        "# Load the tokenizer\n",
        "tokenizer = BartTokenizer.from_pretrained(\"/content/drive/MyDrive/Abreviation/bart5\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d1e65d3-7191-4e89-9aa1-e4904643ddb9",
        "id": "3RSd6Jhp5kmz"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BartForConditionalGeneration(\n",
              "  (model): BartModel(\n",
              "    (shared): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
              "    (encoder): BartEncoder(\n",
              "      (embed_tokens): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
              "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x BartEncoderLayer(\n",
              "          (self_attn): BartSdpaAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (decoder): BartDecoder(\n",
              "      (embed_tokens): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
              "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x BartDecoderLayer(\n",
              "          (self_attn): BartSdpaAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartSdpaAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartForConditionalGeneration\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "model.to(device)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Training and validation loop\n",
        "epochs =1\n",
        "\n",
        "def compute_metrics(preds, labels):\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    precision = precision_score(labels, preds, average='weighted', zero_division=0)\n",
        "    recall = recall_score(labels, preds, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(labels, preds, average='weighted', zero_division=0)\n",
        "    return accuracy, precision, recall, f1\n",
        "max_new_tokens=25\n",
        "for epoch in range(epochs):\n",
        "    # Training phase\n",
        "    print(epoch)\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    j=1\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        #print(j,\"  loss: \",loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        j=j+1\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {train_loss / len(train_loader):.4f}\")\n",
        "    model.save_pretrained(\"/content/drive/MyDrive/Abreviation/bart6\")\n",
        "    tokenizer.save_pretrained(\"/content/drive/MyDrive/Abreviation/bart6\")\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_predictions = []\n",
        "    val_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in valid_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            val_loss += outputs.loss.item()\n",
        "\n",
        "            # Decode predictions and labels for metric calculation\n",
        "            predictions = model.generate(input_ids, attention_mask=attention_mask)\n",
        "            val_predictions.extend([tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions])\n",
        "            val_labels.extend([tokenizer.decode(label, skip_special_tokens=True) for label in labels])\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {val_loss / len(valid_loader):.4f}\")\n",
        "    # Compute validation metrics\n",
        "    metrics = compute_metrics(val_predictions, val_labels)\n",
        "    print(f\"Validation Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDSmqr7D7PTD",
        "outputId": "c4e66ec8-3458-4b3e-cbdf-92a01de9c379"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Epoch 1/1, Training Loss: 0.0005\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, Validation Loss: 0.0006\n",
            "Validation Metrics: Accuracy: 0.8993, Precision: 0.9118, Recall: 0.8993, F1: 0.8923\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb9d9cfe-977d-47cb-d124-a7d117ec935b",
        "id": "pkT3pHhs5km0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, test Loss: 0.0006\n",
            "Test Metrics: Accuracy: 0.9016, Precision: 0.9144, Recall: 0.9016, F1: 0.8965\n"
          ]
        }
      ],
      "source": [
        "# test phase\n",
        "test_dataset = AbbreviationDataset(test_df, tokenizer)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8)\n",
        "model.eval()\n",
        "test_loss = 0\n",
        "test_predictions = []\n",
        "test_labels = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        test_loss += outputs.loss.item()\n",
        "\n",
        "        # Decode predictions and labels for metric calculation\n",
        "        predictions = model.generate(input_ids, attention_mask=attention_mask)\n",
        "        test_predictions.extend([tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions])\n",
        "        test_labels.extend([tokenizer.decode(label, skip_special_tokens=True) for label in labels])\n",
        "\n",
        "print(f\"Epoch {epoch + 1}/{epochs}, test Loss: {test_loss / len(test_loader):.4f}\")\n",
        "# Compute test metrics\n",
        "metrics = compute_metrics(test_predictions, test_labels)\n",
        "print(f\"Test Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train results  bart51\n",
        "model.eval()\n",
        "train_loss = 0\n",
        "train_predictions = []\n",
        "train_labels = []\n",
        "with torch.no_grad():\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        train_loss += outputs.loss.item()\n",
        "\n",
        "        # Decode predictions and labels for metric calculation\n",
        "        predictions = model.generate(input_ids, attention_mask=attention_mask)\n",
        "        train_predictions.extend([tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions])\n",
        "        train_labels.extend([tokenizer.decode(label, skip_special_tokens=True) for label in labels])\n",
        "\n",
        "print(f\"Epoch {epoch + 1}/{epochs}, train Loss: {train_loss / len(train_loader):.4f}\")\n",
        "# Compute train metrics\n",
        "metrics = compute_metrics(train_predictions, train_labels)\n",
        "print(f\"train Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5663c9a-2361-4711-df31-8e6bd31e2898",
        "id": "UCXqAp4x5km1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, train Loss: 0.0002\n",
            "train Metrics: Accuracy: 0.9218, Precision: 0.9411, Recall: 0.9218, F1: 0.9200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartForConditionalGeneration\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "model.to(device)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Training and validation loop\n",
        "epochs =1\n",
        "\n",
        "def compute_metrics(preds, labels):\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    precision = precision_score(labels, preds, average='weighted', zero_division=0)\n",
        "    recall = recall_score(labels, preds, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(labels, preds, average='weighted', zero_division=0)\n",
        "    return accuracy, precision, recall, f1\n",
        "max_new_tokens=25\n",
        "for epoch in range(epochs):\n",
        "    # Training phase\n",
        "    print(epoch)\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    j=1\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        #print(j,\"  loss: \",loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        j=j+1\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {train_loss / len(train_loader):.4f}\")\n",
        "    model.save_pretrained(\"/content/drive/MyDrive/Abreviation/bart6\")\n",
        "    tokenizer.save_pretrained(\"/content/drive/MyDrive/Abreviation/bart6\")\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_predictions = []\n",
        "    val_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in valid_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            val_loss += outputs.loss.item()\n",
        "\n",
        "            # Decode predictions and labels for metric calculation\n",
        "            predictions = model.generate(input_ids, attention_mask=attention_mask)\n",
        "            val_predictions.extend([tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions])\n",
        "            val_labels.extend([tokenizer.decode(label, skip_special_tokens=True) for label in labels])\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {val_loss / len(valid_loader):.4f}\")\n",
        "    # Compute validation metrics\n",
        "    metrics = compute_metrics(val_predictions, val_labels)\n",
        "    print(f\"Validation Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrDgR1EiTBYv",
        "outputId": "346ce1b6-cc22-4612-fc6a-00494c4d32b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Epoch 1/1, Training Loss: 0.0004\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, Validation Loss: 0.0006\n",
            "Validation Metrics: Accuracy: 0.9074, Precision: 0.9300, Recall: 0.9074, F1: 0.9077\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c05c4f3-e9c6-4e6a-c5e0-c201a3d963ab",
        "id": "flJsEz5ZTB6v"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, test Loss: 0.0005\n",
            "Test Metrics: Accuracy: 0.9157, Precision: 0.9325, Recall: 0.9157, F1: 0.9168\n"
          ]
        }
      ],
      "source": [
        "# test phase\n",
        "test_dataset = AbbreviationDataset(test_df, tokenizer)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8)\n",
        "model.eval()\n",
        "test_loss = 0\n",
        "test_predictions = []\n",
        "test_labels = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        test_loss += outputs.loss.item()\n",
        "\n",
        "        # Decode predictions and labels for metric calculation\n",
        "        predictions = model.generate(input_ids, attention_mask=attention_mask)\n",
        "        test_predictions.extend([tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions])\n",
        "        test_labels.extend([tokenizer.decode(label, skip_special_tokens=True) for label in labels])\n",
        "\n",
        "print(f\"Epoch {epoch + 1}/{epochs}, test Loss: {test_loss / len(test_loader):.4f}\")\n",
        "# Compute test metrics\n",
        "metrics = compute_metrics(test_predictions, test_labels)\n",
        "print(f\"Test Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train phase\n",
        "model.eval()\n",
        "train_loss = 0\n",
        "train_predictions = []\n",
        "train_labels = []\n",
        "with torch.no_grad():\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        train_loss += outputs.loss.item()\n",
        "\n",
        "        # Decode predictions and labels for metric calculation\n",
        "        predictions = model.generate(input_ids, attention_mask=attention_mask)\n",
        "        train_predictions.extend([tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions])\n",
        "        train_labels.extend([tokenizer.decode(label, skip_special_tokens=True) for label in labels])\n",
        "\n",
        "print(f\"Epoch {epoch + 1}/{epochs}, train Loss: {train_loss / len(train_loader):.4f}\")\n",
        "# Compute train metrics\n",
        "metrics = compute_metrics(train_predictions, train_labels)\n",
        "print(f\"train Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zy0b5iJaTB6x",
        "outputId": "a2758cc9-0091-4ed9-9690-aeb28d2fd74a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, train Loss: 0.0002\n",
            "train Metrics: Accuracy: 0.9347, Precision: 0.9553, Recall: 0.9347, F1: 0.9375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0bXnlNUn50Xq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7 epoch"
      ],
      "metadata": {
        "id": "Jiw6fniusZnx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartForConditionalGeneration\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "model.to(device)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Training and validation loop\n",
        "epochs =1\n",
        "\n",
        "def compute_metrics(preds, labels):\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    precision = precision_score(labels, preds, average='weighted', zero_division=0)\n",
        "    recall = recall_score(labels, preds, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(labels, preds, average='weighted', zero_division=0)\n",
        "    return accuracy, precision, recall, f1\n",
        "max_new_tokens=25\n",
        "for epoch in range(epochs):\n",
        "    # Training phase\n",
        "    print(epoch)\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    j=1\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        #print(j,\"  loss: \",loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        j=j+1\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {train_loss / len(train_loader):.4f}\")\n",
        "    model.save_pretrained(\"/content/drive/MyDrive/Abreviation/bart51\")\n",
        "    tokenizer.save_pretrained(\"/content/drive/MyDrive/Abreviation/bart51\")\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_predictions = []\n",
        "    val_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in valid_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            val_loss += outputs.loss.item()\n",
        "\n",
        "            # Decode predictions and labels for metric calculation\n",
        "            predictions = model.generate(input_ids, attention_mask=attention_mask)\n",
        "            val_predictions.extend([tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions])\n",
        "            val_labels.extend([tokenizer.decode(label, skip_special_tokens=True) for label in labels])\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {val_loss / len(valid_loader):.4f}\")\n",
        "    # Compute validation metrics\n",
        "    metrics = compute_metrics(val_predictions, val_labels)\n",
        "    print(f\"Validation Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88adb1e7-a801-465e-b332-07fa7edd702d",
        "id": "Dx5Tj5VV52eU"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Epoch 1/1, Training Loss: 0.0003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, Validation Loss: 0.0005\n",
            "Validation Metrics: Accuracy: 0.9177, Precision: 0.9366, Recall: 0.9177, F1: 0.9161\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1119049e-3d4e-4671-bc30-7ed438ebb3ce",
        "id": "jrPYT0mg52eW"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, test Loss: 0.0005\n",
            "Test Metrics: Accuracy: 0.9243, Precision: 0.9393, Recall: 0.9243, F1: 0.9250\n"
          ]
        }
      ],
      "source": [
        "# test phase\n",
        "test_dataset = AbbreviationDataset(test_df, tokenizer)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8)\n",
        "model.eval()\n",
        "test_loss = 0\n",
        "test_predictions = []\n",
        "test_labels = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        test_loss += outputs.loss.item()\n",
        "\n",
        "        # Decode predictions and labels for metric calculation\n",
        "        predictions = model.generate(input_ids, attention_mask=attention_mask)\n",
        "        test_predictions.extend([tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions])\n",
        "        test_labels.extend([tokenizer.decode(label, skip_special_tokens=True) for label in labels])\n",
        "\n",
        "print(f\"Epoch {epoch + 1}/{epochs}, test Loss: {test_loss / len(test_loader):.4f}\")\n",
        "# Compute test metrics\n",
        "metrics = compute_metrics(test_predictions, test_labels)\n",
        "print(f\"Test Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8 epoch"
      ],
      "metadata": {
        "id": "olFsiB2PL94Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train results  bart51\n",
        "model.eval()\n",
        "train_loss = 0\n",
        "train_predictions = []\n",
        "train_labels = []\n",
        "with torch.no_grad():\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        train_loss += outputs.loss.item()\n",
        "\n",
        "        # Decode predictions and labels for metric calculation\n",
        "        predictions = model.generate(input_ids, attention_mask=attention_mask)\n",
        "        train_predictions.extend([tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions])\n",
        "        train_labels.extend([tokenizer.decode(label, skip_special_tokens=True) for label in labels])\n",
        "\n",
        "print(f\"Epoch {epoch + 1}/{epochs}, train Loss: {train_loss / len(train_loader):.4f}\")\n",
        "# Compute train metrics\n",
        "metrics = compute_metrics(train_predictions, train_labels)\n",
        "print(f\"train Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bb7d83c-3ef9-4fb0-b0bf-c419555af723",
        "id": "WYBYpLaA52eW"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, train Loss: 0.0001\n",
            "train Metrics: Accuracy: 0.9467, Precision: 0.9632, Recall: 0.9467, F1: 0.9484\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#########################\"micro\n",
        "# train results  bart51\n",
        "model.eval()\n",
        "train_loss = 0\n",
        "train_predictions = []\n",
        "train_labels = []\n",
        "with torch.no_grad():\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        train_loss += outputs.loss.item()\n",
        "\n",
        "        # Decode predictions and labels for metric calculation\n",
        "        predictions = model.generate(input_ids, attention_mask=attention_mask)\n",
        "        train_predictions.extend([tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions])\n",
        "        train_labels.extend([tokenizer.decode(label, skip_special_tokens=True) for label in labels])\n",
        "\n",
        "print(f\"Epoch {epoch + 1}/{epochs}, train Loss: {train_loss / len(train_loader):.4f}\")\n",
        "# Compute train metrics\n",
        "metrics = compute_metrics(train_predictions, train_labels)\n",
        "print(f\"train Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQzDvc823bwl",
        "outputId": "fbf06c65-7985-45a8-9a55-6e09df385fc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, train Loss: 0.0001\n",
            "train Metrics: Accuracy: 0.9467, Precision: 0.9467, Recall: 0.9467, F1: 0.9467\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "End"
      ],
      "metadata": {
        "id": "BFkIUEFk3bYw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e624d47635b24d7fbc4bfee8e47d0203": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0b17152d3dd24809a4abaecbce794546",
              "IPY_MODEL_7be038234a3f4ac1813769e50610b7e8",
              "IPY_MODEL_53658098b00943778cd9f06f9b17e01d"
            ],
            "layout": "IPY_MODEL_3fa6bf7687df4548a439b76f4de03ad1"
          }
        },
        "0b17152d3dd24809a4abaecbce794546": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b3b6ea2745947b2927ed7156c65b208",
            "placeholder": "",
            "style": "IPY_MODEL_e91a0784e2be4b88bf56b5e55c94119b",
            "value": "vocab.json:100%"
          }
        },
        "7be038234a3f4ac1813769e50610b7e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5db216ff8f44b9f9c005f74cd33bf73",
            "max": 898823,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_89a357c98f29451e9d33750e4701fbe3",
            "value": 898823
          }
        },
        "53658098b00943778cd9f06f9b17e01d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd7a1e44d5a84f37bfa0acb718d371af",
            "placeholder": "",
            "style": "IPY_MODEL_6d799d5343b04ba4ae41628dc0e57c87",
            "value": "899k/899k[00:00&lt;00:00,6.17MB/s]"
          }
        },
        "3fa6bf7687df4548a439b76f4de03ad1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b3b6ea2745947b2927ed7156c65b208": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e91a0784e2be4b88bf56b5e55c94119b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b5db216ff8f44b9f9c005f74cd33bf73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89a357c98f29451e9d33750e4701fbe3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cd7a1e44d5a84f37bfa0acb718d371af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d799d5343b04ba4ae41628dc0e57c87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "52e7b85655e5456a97eb8b45aa74da3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5da6f54e620b46b0ba82747e5023dd69",
              "IPY_MODEL_4990b51eb3ea4d79a0719fffeb2979a5",
              "IPY_MODEL_6b7305eecd5c408d84a0248524712550"
            ],
            "layout": "IPY_MODEL_9497df3847df4bc28d47e9754da1ee89"
          }
        },
        "5da6f54e620b46b0ba82747e5023dd69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63a9755b9434451e8e7ac6eee5a22dc6",
            "placeholder": "",
            "style": "IPY_MODEL_9b0f0c0f685e4682a294a076d89b323e",
            "value": "merges.txt:100%"
          }
        },
        "4990b51eb3ea4d79a0719fffeb2979a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0219075dd1a42a88592c943e50e0f19",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a3f6f5e9fed54b139dbfd1192539d3c3",
            "value": 456318
          }
        },
        "6b7305eecd5c408d84a0248524712550": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6264e882ab9a4048a04f8ca471a30b71",
            "placeholder": "",
            "style": "IPY_MODEL_f858237bad74476488c2a3204565329b",
            "value": "456k/456k[00:00&lt;00:00,3.31MB/s]"
          }
        },
        "9497df3847df4bc28d47e9754da1ee89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63a9755b9434451e8e7ac6eee5a22dc6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b0f0c0f685e4682a294a076d89b323e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d0219075dd1a42a88592c943e50e0f19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3f6f5e9fed54b139dbfd1192539d3c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6264e882ab9a4048a04f8ca471a30b71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f858237bad74476488c2a3204565329b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d7de0ab94470430794a5a46441d55f17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e4e886c2a4da41a1b0ce36839f0028a2",
              "IPY_MODEL_f73318460fd94e0b8a069ecb5818e5bf",
              "IPY_MODEL_03b68632d71f4b78ba6ee4d4095b4fd2"
            ],
            "layout": "IPY_MODEL_b9f40c42baf24412ad1c257aa1187261"
          }
        },
        "e4e886c2a4da41a1b0ce36839f0028a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef899ccc98284be08e03f2b2bca195b2",
            "placeholder": "",
            "style": "IPY_MODEL_d5cff1077ebe43f29c5b624a56d3cdce",
            "value": "tokenizer.json:100%"
          }
        },
        "f73318460fd94e0b8a069ecb5818e5bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56e333a774784208b896326dfcd59f3c",
            "max": 1355863,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c8b5e016891a434ead38359553938745",
            "value": 1355863
          }
        },
        "03b68632d71f4b78ba6ee4d4095b4fd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c43325e30ff347458996e6f9f109919d",
            "placeholder": "",
            "style": "IPY_MODEL_18d025c632ca4741acc16f08d4bb73a5",
            "value": "1.36M/1.36M[00:00&lt;00:00,6.21MB/s]"
          }
        },
        "b9f40c42baf24412ad1c257aa1187261": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef899ccc98284be08e03f2b2bca195b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5cff1077ebe43f29c5b624a56d3cdce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "56e333a774784208b896326dfcd59f3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8b5e016891a434ead38359553938745": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c43325e30ff347458996e6f9f109919d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18d025c632ca4741acc16f08d4bb73a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d2037dd0669b45839c1bd14ddc6ffc05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_199ffff7d4c543448cb1661b6596d1dc",
              "IPY_MODEL_07895e0dde394369a31e76e6c8f15b52",
              "IPY_MODEL_694fc5bf4c41452aa17d318357d2cc50"
            ],
            "layout": "IPY_MODEL_dd7644b6b4d74e118b4468c44b6442bc"
          }
        },
        "199ffff7d4c543448cb1661b6596d1dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_98fea2c6e34c4ba8916f3f1d3b9106ae",
            "placeholder": "",
            "style": "IPY_MODEL_d943537168dc4f559b4f297c0d2d2904",
            "value": "config.json:100%"
          }
        },
        "07895e0dde394369a31e76e6c8f15b52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94d34072a6794bfda260a85ca0c4c38a",
            "max": 1716,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f0e518f21a664db5ae48b1594a11cd2f",
            "value": 1716
          }
        },
        "694fc5bf4c41452aa17d318357d2cc50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58d0b6ce41fb4029889fa133c658de39",
            "placeholder": "",
            "style": "IPY_MODEL_abdb9ce65d5743baa4be687bc2aa67e7",
            "value": "1.72k/1.72k[00:00&lt;00:00,95.6kB/s]"
          }
        },
        "dd7644b6b4d74e118b4468c44b6442bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98fea2c6e34c4ba8916f3f1d3b9106ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d943537168dc4f559b4f297c0d2d2904": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "94d34072a6794bfda260a85ca0c4c38a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0e518f21a664db5ae48b1594a11cd2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "58d0b6ce41fb4029889fa133c658de39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abdb9ce65d5743baa4be687bc2aa67e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}