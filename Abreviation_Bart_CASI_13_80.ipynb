{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "L0r1wqUG9Nwf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4T_klvvlUPu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "342ed308-bde2-42a2-a0c2-add601999956"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Check GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUPLnM7mIt0v",
        "outputId": "9ea43eae-d85a-4a9e-e3dc-d81582f8b5e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xhi1qk0lGy9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce785d4d-0669-4a5d-dbf4-3717a98ceec5",
        "id": "zaqvSry4G2zW"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       abr                                      long abr_text  start_abr  \\\n",
            "0       AB                                  abortion      AB.        231   \n",
            "1       AB                                  abortion      AB.        249   \n",
            "2       AB                                  abortion       AB        223   \n",
            "3       AB                                  abortion      AB.        194   \n",
            "4       AB                                  abortion       AB        114   \n",
            "...    ...                                       ...      ...        ...   \n",
            "37495  VAD  vincristine adriamycin and dexamethasone     VAD.        139   \n",
            "37496  VAD  vincristine adriamycin and dexamethasone     VAD,        172   \n",
            "37497  VAD  vincristine adriamycin and dexamethasone      VAD        250   \n",
            "37498  VAD  vincristine adriamycin and dexamethasone      VAD        181   \n",
            "37499  VAD  vincristine adriamycin and dexamethasone      VAD         60   \n",
            "\n",
            "       end_abr                         section  \\\n",
            "0          233                                   \n",
            "1          251                                   \n",
            "2          224                 PAST OB HISTORY   \n",
            "3          196  HISTORY OF THE PRESENT ILLNESS   \n",
            "4          115             PAST OB-GYN HISTORY   \n",
            "...        ...                             ...   \n",
            "37495      142            PAST MEDICAL HISTORY   \n",
            "37496      175                      IMPRESSION   \n",
            "37497      252      HISTORY OF PRESENT ILLNESS   \n",
            "37498      183      HISTORY OF PRESENT ILLNESS   \n",
            "37499       62                      PROCEDURES   \n",
            "\n",
            "                                                    text  \n",
            "0      _%#NAME#%_ _%#NAME#%_ is a 29-year-old gravida...  \n",
            "1      She is now bleeding quite heavily. Ultrasound ...  \n",
            "2      ALLERGIES: Heparin and Imitrex. PAST OB HISTOR...  \n",
            "3      She had a pelvic ultrasound at Park Nicollet o...  \n",
            "4      On _%#MMDD2007#%_, normal anatomy with anterio...  \n",
            "...                                                  ...  \n",
            "37495  1. Multiple myeloma, undergoing chemotherapy. ...  \n",
            "37496  He has been receiving weekly Procrit injection...  \n",
            "37497  Within a month, he developed recurrent hip pai...  \n",
            "37498  He had a serum protein electrophoresis with a ...  \n",
            "37499  DISCHARGE DIAGNOSES: Multiple myeloma. PROCEDU...  \n",
            "\n",
            "[37500 rows x 7 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Initialize an empty list to store data for the DataFrame\n",
        "data = []\n",
        "\n",
        "# Define the path to your text file\n",
        "file_path = '/content/drive/MyDrive/Abreviation/CASI/CASI.txt'\n",
        "\n",
        "# Read the file line by line\n",
        "with open(file_path, 'r',encoding='utf-8', errors='ignore') as file:\n",
        "  for line in file:\n",
        "    # Split the line by the '|' delimiter\n",
        "    parts = line.split('|')\n",
        "\n",
        "    # Extract elements based on the expected format\n",
        "    if len(parts) >= 7:  # Ensure the line has the expected number of fields\n",
        "        abr = parts[0]\n",
        "        long_form = parts[1]\n",
        "        abr_text = parts[2]\n",
        "        start_abr = int(parts[3])\n",
        "        end_abr = int(parts[4])\n",
        "        section = parts[5]  # May be empty if no section is specified\n",
        "        text = parts[6]\n",
        "\n",
        "        # Append the extracted fields to the data list as a dictionary\n",
        "        data.append({\n",
        "            'abr': abr,\n",
        "            'long': long_form,\n",
        "            'abr_text': abr_text,\n",
        "            'start_abr': start_abr,\n",
        "            'end_abr': end_abr,\n",
        "            'section': section,\n",
        "            'text': text\n",
        "        })\n",
        "\n",
        "# Convert the list of dictionaries to a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Display the resulting DataFrame\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQ4jNGPXG2zX"
      },
      "outputs": [],
      "source": [
        "data=pd.DataFrame(df, columns=('abr','long','text'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the target abbreviations\n",
        "df=data\n",
        "abbreviations = [\"AMA\", \"ASA\", \"BAL\",\"BK\",\"C3\",\"CVA\",\"CVP\",\"CVS\",\"ER\",\"FISH\",\"NAD\",\"OTC\",\"SBP\"]\n",
        "\n",
        "# Filter rows containing any of the abbreviations\n",
        "filtered_df = df[df[\"abr\"].str.contains('|'.join(abbreviations), flags=re.IGNORECASE, na=False)]\n",
        "\n",
        "# Sample 500 rows from the filtered data\n",
        "#df_mini = filtered_df.sample(n=500, replace=False, random_state=1992)\n"
      ],
      "metadata": {
        "id": "JIi4buTkUHGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the distribution of abbreviations in the sampled data\n",
        "print(filtered_df['abr'].value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a016c1f3-755c-4809-df41-3e89cf6e2b29",
        "id": "8saE-3UdG2zY"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "abr\n",
            "AMA     500\n",
            "ASA     500\n",
            "BAL     500\n",
            "BK      500\n",
            "C3      500\n",
            "CVA     500\n",
            "CVP     500\n",
            "CVS     500\n",
            "ER      500\n",
            "FISH    500\n",
            "NAD     500\n",
            "OTC     500\n",
            "SBP     500\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the distribution of abbreviations in the sampled data\n",
        "data=filtered_df\n",
        "print(data['abr'].value_counts())\n",
        "print(data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNO7luNVdzZb",
        "outputId": "ac04583a-057d-4279-c0b6-a27a5b7d2192"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "abr\n",
            "AMA     500\n",
            "ASA     500\n",
            "BAL     500\n",
            "BK      500\n",
            "C3      500\n",
            "CVA     500\n",
            "CVP     500\n",
            "CVS     500\n",
            "ER      500\n",
            "FISH    500\n",
            "NAD     500\n",
            "OTC     500\n",
            "SBP     500\n",
            "Name: count, dtype: int64\n",
            "(6500, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yw9zABfE89sT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304,
          "referenced_widgets": [
            "62f32d24e3d847c399d53fef502e8347",
            "4772f08870204f64b90aa60a6f8a08a4",
            "854b11dea3af4b22bde63980a50a7d5e",
            "f0202ff2438848a1876abeea7db35849",
            "c917cbbdf96e43dba643c0922d672ecc",
            "5b751878b681422d95dc42010e58530f",
            "bad9f30feae54d95bb7dcff3560c4890",
            "d34852cd3ec74aba9f4252dd95229f79",
            "abfdef37e6524a0a8a88f05e3458b18b",
            "6cdc00d94be4457b8e184cc85905aac1",
            "98d85f91ec614212b44eb849bb9c48b2",
            "df692ebf26c24363a93c214fd964ce2a",
            "cdecb8541567431197dcd0f34b0ad8c2",
            "700b61dc396a456a8c16093a55717d5d",
            "93cb40852edf41399a763b42141d2d81",
            "7a5302253e82448f98d43a7bae9c8793",
            "9b4b64b5825c4f65bf180cc3c58f117a",
            "a4aeb8df54824d648a6d8391ff0853eb",
            "e3936d27e7d546a986123b21debcb14d",
            "f5f512efd49a482ba07e485846004196",
            "1460b7a91f9347cfa13e73f0c588ce02",
            "ec230163e0f24c39b5680d11cf5e1885",
            "111b5f52abac48bfa4b67af55d5fcb73",
            "175029bb4f8145e3bfe1f1e965971246",
            "19056a00e2fc4626982ef99697718c59",
            "3e399096383f46d9816a9423fb4b63ca",
            "359905773a9740778f3eb0f2a76b37b7",
            "05a63f61e6b54fbb8e67eb6da2d6fb2d",
            "448952a091b243acb267809b6c434c4e",
            "fc3998a0654e4651b15b5e0de653df95",
            "e36d1b1253d349059437046492664e6f",
            "56391f1b3651403eb2f770092c4c8d9b",
            "2d3e20b84e7d48baacc395efb4991d35",
            "acc4526414d6486083b26a1ba8e5da20",
            "343561b31a4c44d391810591fce60a61",
            "61c89ea7b98c499f96ea6f15a4e6927a",
            "89f55a4462d04cc9be17b91d6e09a760",
            "96bd31ada74e4b399245691fc9c247f7",
            "442b634729324a7bba7ed4c0833d4027",
            "4845bdb13f42433189b38147f9f6c6d5",
            "6da71279c1a2460eb7b2b08789119a2d",
            "cea6f0a34e0e436090a2be7cbd1408c5",
            "d472d11df7d04923bd57468ddba3e73f",
            "8004a1892ae94e9f9ec281ed728c2abe"
          ]
        },
        "outputId": "e3fd4eea-aa3a-4ddf-b631-3b1db800e31c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "62f32d24e3d847c399d53fef502e8347"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "df692ebf26c24363a93c214fd964ce2a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "111b5f52abac48bfa4b67af55d5fcb73"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.72k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "acc4526414d6486083b26a1ba8e5da20"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(30000, 3)\n",
            "(7500, 3)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BartTokenizer\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "# Tokenizer\n",
        "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
        "def split_into_chunks(text, tokenizer, chunk_size=512):\n",
        "    # Ensure the input is a string, not a list\n",
        "    if isinstance(text, list):\n",
        "        text = \" \".join(text)  # Join the list elements into a single string\n",
        "\n",
        "    tokens = tokenizer.tokenize(text)  # Tokenize the text\n",
        "    return [\" \".join(tokens[i:i + chunk_size]) for i in range(0, len(tokens), chunk_size)]\n",
        "\n",
        "class AbbreviationDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_length=512, chunk_size=512):\n",
        "        self.dataframe = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.chunk_size = chunk_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.dataframe.iloc[idx]\n",
        "        input_text = f\"{row['abr']} - {row['text']}\"\n",
        "        target_text = row[\"long\"]\n",
        "\n",
        "        # Split the input text into chunks\n",
        "        chunks = split_into_chunks(input_text, self.tokenizer, chunk_size=self.chunk_size)\n",
        "\n",
        "        # Tokenize input chunks and target text\n",
        "        inputs = self.tokenizer(\n",
        "            \" \".join(chunks),  # Join the chunks back into a string\n",
        "            max_length=self.max_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        targets = self.tokenizer(\n",
        "            target_text,\n",
        "            max_length=self.max_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": inputs[\"input_ids\"].squeeze(0).to(device),\n",
        "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0).to(device),\n",
        "            \"labels\": targets[\"input_ids\"].squeeze(0).to(device),\n",
        "        }\n",
        "train_df = data.sample(frac=0.80, random_state=42)\n",
        "test_df= data.drop(train_df.index)\n",
        "print(train_df.shape)\n",
        "print(test_df.shape)\n",
        "train_dataset = AbbreviationDataset(train_df, tokenizer)\n",
        "test_dataset = AbbreviationDataset(test_df, tokenizer)\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
        "\n",
        "# Load model and move to device\n",
        "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\").to(device)\n",
        "\n"
      ],
      "metadata": {
        "id": "ruU0UxZqZLe6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301,
          "referenced_widgets": [
            "9082ecb6a42c43458d3ce14cfee77b70",
            "cdb26fb2875f495c8ab6ead0b76041c8",
            "2a45c40b886e4ae3b3606f7c6433a0c4",
            "40bdc431282e4caba101c6269ba4e869",
            "4ab30a7bc7dc4359b3ab0c700e8a35de",
            "f01c7b00a72447fcade67154ca8a4b43",
            "f35911cdd19d4f01a29b91d7937493dc",
            "7400f60d39a64897ad83859d5d1d0f0e",
            "b725bfd735ac4bf8be68c1080f411858",
            "cf37322dcc234bb58cc199fc3054efdc",
            "f323b90f5e244688b4438b28e2e5fc23",
            "3c1f7924b04c4ee8bf108a21997003ab",
            "b5ecfaab13ad443b89c25d1fd874c179",
            "7db040cc57264ac884e86eef3a6089f0",
            "cd8c54cc6bf2463294349f2731012182",
            "8359effe92264b6f92ddcf6596fc3002",
            "fe2c0e2a04664438bacde8d5142321e7",
            "f707e277ee444e269834a392040badbe",
            "0697e0c9bfc84041827763dd3093cc10",
            "d8cfbbda45534c25ac163818725c7259",
            "7558f64016654e2fbdec8367ebaa0c3d",
            "327d1d141660446180fc61a830a9e338",
            "2c40b12e08bd4829baf14923e5138670",
            "578633dde09c4f699cf72b10e74f7dd9",
            "41deb1bf40ee4b45aa339779f4df9c49",
            "8da459e953df4cf180e4ed25c4bd6982",
            "a47acd007293485d95365db0cc197bec",
            "bf776fd69e1640acb4b7d905034dd478",
            "d7871c4fbf174f1d9773d9d06ab2f5da",
            "c198d140e77e45baa30eefeaa15c1555",
            "ff16828b7e8d479d9c853c98c1c0ed68",
            "409dd1e097be4706b3c935c43dfe3d99",
            "ef62a051dd0d414497f1dbfb476ed21f",
            "c440caa9c7ea46958c8ae2063f3a04e1",
            "3a6b7fe39bca4b59a26383d82d62d0e3",
            "a07c94c368ad4ae88e44c3f3779c2e72",
            "4f0f86da0839470f845d38438e24169e",
            "5421f5356b2e4266b574eb3fadd6eb97",
            "695f3eb7f7d5402e87832556cf538f4a",
            "db8f6e71570940649eea413cec8e0305",
            "8666c97565e04473be8f62496640449e",
            "ba26dd6b0cfc43fd876602557c4a5f3d",
            "9e332312332b460481ce45a436a601fb",
            "5219b5750c2a4986a6f118d1ada44379",
            "83d708d5e76744be882f377a955b7536",
            "e8b2208e7db74b4b9973c5dfa3c6b81e",
            "3febdcdbe9e8464a850b0c88f63347c0",
            "a5e434edf14e41f0b0fbf35b4a8ea232",
            "14a10ed1328b4e19a697f5f5474e6d60",
            "b7162a3bc22b45b0b07032fa7d6b246e",
            "3ffa79bc84ec43a284b0880ed435a4bb",
            "8c36ed303b05403e9b5b9ef53b3f6594",
            "0bc289c4f7dc4d23aad174e31632cac2",
            "8dde06a767c34369b6fcb0a388fbf62a",
            "ecd299eec28e4b1894921b2c3361eb4e"
          ]
        },
        "outputId": "a7ff97aa-0ec2-47c3-de1c-5004197435af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9082ecb6a42c43458d3ce14cfee77b70"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3c1f7924b04c4ee8bf108a21997003ab"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2c40b12e08bd4829baf14923e5138670"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.72k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c440caa9c7ea46958c8ae2063f3a04e1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "83d708d5e76744be882f377a955b7536"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAAriPIiEhJ0",
        "outputId": "06c07268-21f9-41cb-c064-8717efdbdf0e"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "Epoch 1/5, Training Loss: 1.7524\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train results\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5, train Loss: 0.0348\n",
            "train Metrics: Accuracy: 0.5742, Precision: 0.5742, Recall: 0.5742, F1: 0.5742\n",
            "test results\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5, Validation Loss: 0.0351\n",
            "Validation Metrics: Accuracy: 0.5577, Precision: 0.5577, Recall: 0.5577, F1: 0.5577\n",
            "1\n",
            "Epoch 2/5, Training Loss: 0.0315\n",
            "train results\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/5, train Loss: 0.0089\n",
            "train Metrics: Accuracy: 0.6935, Precision: 0.6935, Recall: 0.6935, F1: 0.6935\n",
            "test results\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/5, Validation Loss: 0.0091\n",
            "Validation Metrics: Accuracy: 0.7015, Precision: 0.7015, Recall: 0.7015, F1: 0.7015\n",
            "2\n",
            "Epoch 3/5, Training Loss: 0.0114\n",
            "train results\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/5, train Loss: 0.0043\n",
            "train Metrics: Accuracy: 0.7352, Precision: 0.7352, Recall: 0.7352, F1: 0.7352\n",
            "test results\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/5, Validation Loss: 0.0044\n",
            "Validation Metrics: Accuracy: 0.7315, Precision: 0.7315, Recall: 0.7315, F1: 0.7315\n",
            "3\n",
            "Epoch 4/5, Training Loss: 0.0061\n",
            "train results\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/5, train Loss: 0.0026\n",
            "train Metrics: Accuracy: 0.7431, Precision: 0.7431, Recall: 0.7431, F1: 0.7431\n",
            "test results\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5, Validation Loss: 0.0027\n",
            "Validation Metrics: Accuracy: 0.7315, Precision: 0.7315, Recall: 0.7315, F1: 0.7315\n",
            "4\n",
            "Epoch 5/5, Training Loss: 0.0038\n",
            "train results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5, train Loss: 0.0018\n",
            "train Metrics: Accuracy: 0.7402, Precision: 0.7402, Recall: 0.7402, F1: 0.7402\n",
            "test results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5, Validation Loss: 0.0019\n",
            "Validation Metrics: Accuracy: 0.7285, Precision: 0.7285, Recall: 0.7285, F1: 0.7285\n"
          ]
        }
      ],
      "source": [
        "#ok\n",
        "from transformers import BartForConditionalGeneration\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load model\n",
        "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n",
        "model.to(device)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Training and validation loop\n",
        "epochs = 5\n",
        "\n",
        "def compute_metrics(preds, labels):\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    precision = precision_score(labels, preds, average='micro', zero_division=0)\n",
        "    recall = recall_score(labels, preds, average='micro', zero_division=0)\n",
        "    f1 = f1_score(labels, preds, average='micro', zero_division=0)\n",
        "    return accuracy, precision, recall, f1\n",
        "max_new_tokens=25\n",
        "for epoch in range(epochs):\n",
        "    # Training phase\n",
        "    print(epoch)\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    j=1\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        #print(j,\"  loss: \",loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        j=j+1\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {train_loss / len(train_loader):.4f}\")\n",
        "    model.save_pretrained(\"/content/drive/MyDrive/Abreviation/bart\")\n",
        "    tokenizer.save_pretrained(\"/content/drive/MyDrive/Abreviation/bart\")\n",
        "    print('train results')\n",
        "    model.eval()\n",
        "    train_loss = 0\n",
        "    train_predictions = []\n",
        "    train_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in train_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            train_loss += outputs.loss.item()\n",
        "\n",
        "            # Decode predictions and labels for metric calculation\n",
        "            predictions = model.generate(input_ids, attention_mask=attention_mask)\n",
        "            train_predictions.extend([tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions])\n",
        "            train_labels.extend([tokenizer.decode(label, skip_special_tokens=True) for label in labels])\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, train Loss: {train_loss / len(train_loader):.4f}\")\n",
        "    # Compute train metrics\n",
        "    metrics = compute_metrics(train_predictions, train_labels)\n",
        "    print(f\"train Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n",
        "\n",
        "    print('test results')\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_predictions = []\n",
        "    val_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            val_loss += outputs.loss.item()\n",
        "            # Decode predictions and labels for metric calculation\n",
        "            predictions = model.generate(input_ids, attention_mask=attention_mask)\n",
        "            val_predictions.extend([tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions])\n",
        "            val_labels.extend([tokenizer.decode(label, skip_special_tokens=True) for label in labels])\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {val_loss / len(test_loader):.4f}\")\n",
        "    # Compute validation metrics\n",
        "    metrics = compute_metrics(val_predictions, val_labels)\n",
        "    print(f\"Validation Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ok\n",
        "from transformers import BartForConditionalGeneration\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Training and validation loop\n",
        "epochs = 1\n",
        "\n",
        "def compute_metrics(preds, labels):\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    precision = precision_score(labels, preds, average='micro', zero_division=0)\n",
        "    recall = recall_score(labels, preds, average='micro', zero_division=0)\n",
        "    f1 = f1_score(labels, preds, average='micro', zero_division=0)\n",
        "    return accuracy, precision, recall, f1\n",
        "max_new_tokens=25\n",
        "for epoch in range(epochs):\n",
        "    # Training phase\n",
        "    print(epoch)\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    j=1\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        #print(j,\"  loss: \",loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        j=j+1\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {train_loss / len(train_loader):.4f}\")\n",
        "\n",
        "    print('train results')\n",
        "    model.eval()\n",
        "    train_loss = 0\n",
        "    train_predictions = []\n",
        "    train_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in train_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            train_loss += outputs.loss.item()\n",
        "\n",
        "            # Decode predictions and labels for metric calculation\n",
        "            predictions = model.generate(input_ids, attention_mask=attention_mask)\n",
        "            train_predictions.extend([tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions])\n",
        "            train_labels.extend([tokenizer.decode(label, skip_special_tokens=True) for label in labels])\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, train Loss: {train_loss / len(train_loader):.4f}\")\n",
        "    # Compute train metrics\n",
        "    metrics = compute_metrics(train_predictions, train_labels)\n",
        "    print(f\"train Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n",
        "\n",
        "    print('test results')\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_predictions = []\n",
        "    val_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            val_loss += outputs.loss.item()\n",
        "            # Decode predictions and labels for metric calculation\n",
        "            predictions = model.generate(input_ids, attention_mask=attention_mask)\n",
        "            val_predictions.extend([tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions])\n",
        "            val_labels.extend([tokenizer.decode(label, skip_special_tokens=True) for label in labels])\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {val_loss / len(test_loader):.4f}\")\n",
        "    # Compute validation metrics\n",
        "    metrics = compute_metrics(val_predictions, val_labels)\n",
        "    print(f\"Validation Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0AGzjNI9LLm",
        "outputId": "4075e1e5-3712-4efe-eee6-badc6e96d432"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Epoch 1/1, Training Loss: 0.0072\n",
            "train results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, train Loss: 0.0012\n",
            "train Metrics: Accuracy: 0.7915, Precision: 0.7915, Recall: 0.7915, F1: 0.7915\n",
            "test results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, Validation Loss: 0.0014\n",
            "Validation Metrics: Accuracy: 0.7800, Precision: 0.7800, Recall: 0.7800, F1: 0.7800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ok\n",
        "from transformers import BartForConditionalGeneration\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Training and validation loop\n",
        "epochs = 1\n",
        "\n",
        "def compute_metrics(preds, labels):\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    precision = precision_score(labels, preds, average='micro', zero_division=0)\n",
        "    recall = recall_score(labels, preds, average='micro', zero_division=0)\n",
        "    f1 = f1_score(labels, preds, average='micro', zero_division=0)\n",
        "    return accuracy, precision, recall, f1\n",
        "max_new_tokens=25\n",
        "for epoch in range(epochs):\n",
        "    # Training phase\n",
        "    print(epoch)\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    j=1\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        #print(j,\"  loss: \",loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        j=j+1\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {train_loss / len(train_loader):.4f}\")\n",
        "\n",
        "    print('train results')\n",
        "    model.eval()\n",
        "    train_loss = 0\n",
        "    train_predictions = []\n",
        "    train_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in train_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            train_loss += outputs.loss.item()\n",
        "\n",
        "            # Decode predictions and labels for metric calculation\n",
        "            predictions = model.generate(input_ids, attention_mask=attention_mask)\n",
        "            train_predictions.extend([tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions])\n",
        "            train_labels.extend([tokenizer.decode(label, skip_special_tokens=True) for label in labels])\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, train Loss: {train_loss / len(train_loader):.4f}\")\n",
        "    # Compute train metrics\n",
        "    metrics = compute_metrics(train_predictions, train_labels)\n",
        "    print(f\"train Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n",
        "\n",
        "    print('test results')\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_predictions = []\n",
        "    val_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            val_loss += outputs.loss.item()\n",
        "            # Decode predictions and labels for metric calculation\n",
        "            predictions = model.generate(input_ids, attention_mask=attention_mask)\n",
        "            val_predictions.extend([tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions])\n",
        "            val_labels.extend([tokenizer.decode(label, skip_special_tokens=True) for label in labels])\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {val_loss / len(test_loader):.4f}\")\n",
        "    # Compute validation metrics\n",
        "    metrics = compute_metrics(val_predictions, val_labels)\n",
        "    print(f\"Validation Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDxWmcq9bLST",
        "outputId": "d444b533-aa38-4349-c407-465fcd7a73bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Epoch 1/1, Training Loss: 0.0013\n",
            "train results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, train Loss: 0.0005\n",
            "train Metrics: Accuracy: 0.8871, Precision: 0.8871, Recall: 0.8871, F1: 0.8871\n",
            "test results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, Validation Loss: 0.0008\n",
            "Validation Metrics: Accuracy: 0.8685, Precision: 0.8685, Recall: 0.8685, F1: 0.8685\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ok\n",
        "from transformers import BartForConditionalGeneration\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Training and validation loop\n",
        "epochs = 1\n",
        "\n",
        "def compute_metrics(preds, labels):\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    precision = precision_score(labels, preds, average='micro', zero_division=0)\n",
        "    recall = recall_score(labels, preds, average='micro', zero_division=0)\n",
        "    f1 = f1_score(labels, preds, average='micro', zero_division=0)\n",
        "    return accuracy, precision, recall, f1\n",
        "max_new_tokens=25\n",
        "for epoch in range(epochs):\n",
        "    # Training phase\n",
        "    print(epoch)\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    j=1\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        #print(j,\"  loss: \",loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        j=j+1\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {train_loss / len(train_loader):.4f}\")\n",
        "\n",
        "    print('train results')\n",
        "    model.eval()\n",
        "    train_loss = 0\n",
        "    train_predictions = []\n",
        "    train_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in train_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            train_loss += outputs.loss.item()\n",
        "\n",
        "            # Decode predictions and labels for metric calculation\n",
        "            predictions = model.generate(input_ids, attention_mask=attention_mask)\n",
        "            train_predictions.extend([tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions])\n",
        "            train_labels.extend([tokenizer.decode(label, skip_special_tokens=True) for label in labels])\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, train Loss: {train_loss / len(train_loader):.4f}\")\n",
        "    # Compute train metrics\n",
        "    metrics = compute_metrics(train_predictions, train_labels)\n",
        "    print(f\"train Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n",
        "\n",
        "    print('test results')\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_predictions = []\n",
        "    val_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            val_loss += outputs.loss.item()\n",
        "            # Decode predictions and labels for metric calculation\n",
        "            predictions = model.generate(input_ids, attention_mask=attention_mask)\n",
        "            val_predictions.extend([tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions])\n",
        "            val_labels.extend([tokenizer.decode(label, skip_special_tokens=True) for label in labels])\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {val_loss / len(test_loader):.4f}\")\n",
        "    # Compute validation metrics\n",
        "    metrics = compute_metrics(val_predictions, val_labels)\n",
        "    print(f\"Validation Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qf330g0Pg2qM",
        "outputId": "81e0739e-6364-4187-afba-e17e3e279a52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Epoch 1/1, Training Loss: 0.0008\n",
            "train results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, train Loss: 0.0002\n",
            "train Metrics: Accuracy: 0.9279, Precision: 0.9279, Recall: 0.9279, F1: 0.9279\n",
            "test results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, Validation Loss: 0.0003\n",
            "Validation Metrics: Accuracy: 0.9200, Precision: 0.9200, Recall: 0.9200, F1: 0.9200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ok\n",
        "from transformers import BartForConditionalGeneration\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Training and validation loop\n",
        "epochs = 1\n",
        "\n",
        "def compute_metrics(preds, labels):\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    precision = precision_score(labels, preds, average='micro', zero_division=0)\n",
        "    recall = recall_score(labels, preds, average='micro', zero_division=0)\n",
        "    f1 = f1_score(labels, preds, average='micro', zero_division=0)\n",
        "    return accuracy, precision, recall, f1\n",
        "max_new_tokens=25\n",
        "for epoch in range(epochs):\n",
        "    # Training phase\n",
        "    print(epoch)\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    j=1\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        #print(j,\"  loss: \",loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        j=j+1\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {train_loss / len(train_loader):.4f}\")\n",
        "\n",
        "    print('train results')\n",
        "    model.eval()\n",
        "    train_loss = 0\n",
        "    train_predictions = []\n",
        "    train_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in train_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            train_loss += outputs.loss.item()\n",
        "\n",
        "            # Decode predictions and labels for metric calculation\n",
        "            predictions = model.generate(input_ids, attention_mask=attention_mask)\n",
        "            train_predictions.extend([tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions])\n",
        "            train_labels.extend([tokenizer.decode(label, skip_special_tokens=True) for label in labels])\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, train Loss: {train_loss / len(train_loader):.4f}\")\n",
        "    # Compute train metrics\n",
        "    metrics = compute_metrics(train_predictions, train_labels)\n",
        "    print(f\"train Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n",
        "\n",
        "    print('test results')\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_predictions = []\n",
        "    val_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            val_loss += outputs.loss.item()\n",
        "            # Decode predictions and labels for metric calculation\n",
        "            predictions = model.generate(input_ids, attention_mask=attention_mask)\n",
        "            val_predictions.extend([tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions])\n",
        "            val_labels.extend([tokenizer.decode(label, skip_special_tokens=True) for label in labels])\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {val_loss / len(test_loader):.4f}\")\n",
        "    # Compute validation metrics\n",
        "    metrics = compute_metrics(val_predictions, val_labels)\n",
        "    print(f\"Validation Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8735cb24-8f08-4ae7-d0f9-ce6baac00931",
        "id": "oGQjwD5jm24B"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Epoch 1/1, Training Loss: 0.0005\n",
            "train results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, train Loss: 0.0001\n",
            "train Metrics: Accuracy: 0.9513, Precision: 0.9513, Recall: 0.9513, F1: 0.9513\n",
            "test results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, Validation Loss: 0.0003\n",
            "Validation Metrics: Accuracy: 0.9331, Precision: 0.9331, Recall: 0.9331, F1: 0.9331\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ok\n",
        "from transformers import BartForConditionalGeneration\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Training and validation loop\n",
        "epochs = 1\n",
        "\n",
        "def compute_metrics(preds, labels):\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    precision = precision_score(labels, preds, average='micro', zero_division=0)\n",
        "    recall = recall_score(labels, preds, average='micro', zero_division=0)\n",
        "    f1 = f1_score(labels, preds, average='micro', zero_division=0)\n",
        "    return accuracy, precision, recall, f1\n",
        "max_new_tokens=25\n",
        "for epoch in range(epochs):\n",
        "    # Training phase\n",
        "    print(epoch)\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    j=1\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        #print(j,\"  loss: \",loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        j=j+1\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {train_loss / len(train_loader):.4f}\")\n",
        "\n",
        "    print('train results')\n",
        "    model.eval()\n",
        "    train_loss = 0\n",
        "    train_predictions = []\n",
        "    train_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in train_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            train_loss += outputs.loss.item()\n",
        "\n",
        "            # Decode predictions and labels for metric calculation\n",
        "            predictions = model.generate(input_ids, attention_mask=attention_mask)\n",
        "            train_predictions.extend([tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions])\n",
        "            train_labels.extend([tokenizer.decode(label, skip_special_tokens=True) for label in labels])\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, train Loss: {train_loss / len(train_loader):.4f}\")\n",
        "    # Compute train metrics\n",
        "    metrics = compute_metrics(train_predictions, train_labels)\n",
        "    print(f\"train Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n",
        "\n",
        "    print('test results')\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_predictions = []\n",
        "    val_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            val_loss += outputs.loss.item()\n",
        "            # Decode predictions and labels for metric calculation\n",
        "            predictions = model.generate(input_ids, attention_mask=attention_mask)\n",
        "            val_predictions.extend([tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions])\n",
        "            val_labels.extend([tokenizer.decode(label, skip_special_tokens=True) for label in labels])\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {val_loss / len(test_loader):.4f}\")\n",
        "    # Compute validation metrics\n",
        "    metrics = compute_metrics(val_predictions, val_labels)\n",
        "    print(f\"Validation Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26addb68-e073-4ef2-a801-21559ca87b29",
        "id": "ts1DAVyGtjEc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Epoch 1/1, Training Loss: 0.0004\n",
            "train results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, train Loss: 0.0001\n",
            "train Metrics: Accuracy: 0.9475, Precision: 0.9475, Recall: 0.9475, F1: 0.9475\n",
            "test results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, Validation Loss: 0.0003\n",
            "Validation Metrics: Accuracy: 0.9269, Precision: 0.9269, Recall: 0.9269, F1: 0.9269\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ok\n",
        "from transformers import BartForConditionalGeneration\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Training and validation loop\n",
        "epochs = 1\n",
        "\n",
        "def compute_metrics(preds, labels):\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    precision = precision_score(labels, preds, average='micro', zero_division=0)\n",
        "    recall = recall_score(labels, preds, average='micro', zero_division=0)\n",
        "    f1 = f1_score(labels, preds, average='micro', zero_division=0)\n",
        "    return accuracy, precision, recall, f1\n",
        "max_new_tokens=25\n",
        "for epoch in range(epochs):\n",
        "    # Training phase\n",
        "    print(epoch)\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    j=1\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        #print(j,\"  loss: \",loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        j=j+1\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {train_loss / len(train_loader):.4f}\")\n",
        "\n",
        "    print('train results')\n",
        "    model.eval()\n",
        "    train_loss = 0\n",
        "    train_predictions = []\n",
        "    train_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in train_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            train_loss += outputs.loss.item()\n",
        "\n",
        "            # Decode predictions and labels for metric calculation\n",
        "            predictions = model.generate(input_ids, attention_mask=attention_mask)\n",
        "            train_predictions.extend([tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions])\n",
        "            train_labels.extend([tokenizer.decode(label, skip_special_tokens=True) for label in labels])\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, train Loss: {train_loss / len(train_loader):.4f}\")\n",
        "    # Compute train metrics\n",
        "    metrics = compute_metrics(train_predictions, train_labels)\n",
        "    print(f\"train Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n",
        "\n",
        "    print('test results')\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_predictions = []\n",
        "    val_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            val_loss += outputs.loss.item()\n",
        "            # Decode predictions and labels for metric calculation\n",
        "            predictions = model.generate(input_ids, attention_mask=attention_mask)\n",
        "            val_predictions.extend([tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions])\n",
        "            val_labels.extend([tokenizer.decode(label, skip_special_tokens=True) for label in labels])\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {val_loss / len(test_loader):.4f}\")\n",
        "    # Compute validation metrics\n",
        "    metrics = compute_metrics(val_predictions, val_labels)\n",
        "    print(f\"Validation Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0VcPUDoz9Pn",
        "outputId": "3ef27f26-2da2-4951-b860-be27d1b7380e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Epoch 1/1, Training Loss: 0.0002\n",
            "train results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, train Loss: 0.0001\n",
            "train Metrics: Accuracy: 0.9608, Precision: 0.9608, Recall: 0.9608, F1: 0.9608\n",
            "test results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ok\n",
        "from transformers import BartForConditionalGeneration\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Training and validation loop\n",
        "epochs = 1\n",
        "\n",
        "def compute_metrics(preds, labels):\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    precision = precision_score(labels, preds, average='micro', zero_division=0)\n",
        "    recall = recall_score(labels, preds, average='micro', zero_division=0)\n",
        "    f1 = f1_score(labels, preds, average='micro', zero_division=0)\n",
        "    return accuracy, precision, recall, f1\n",
        "max_new_tokens=25\n",
        "for epoch in range(epochs):\n",
        "    # Training phase\n",
        "    print(epoch)\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    j=1\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        #print(j,\"  loss: \",loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        j=j+1\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {train_loss / len(train_loader):.4f}\")\n",
        "\n",
        "    print('train results')\n",
        "    model.eval()\n",
        "    train_loss = 0\n",
        "    train_predictions = []\n",
        "    train_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in train_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            train_loss += outputs.loss.item()\n",
        "\n",
        "            # Decode predictions and labels for metric calculation\n",
        "            predictions = model.generate(input_ids, attention_mask=attention_mask)\n",
        "            train_predictions.extend([tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions])\n",
        "            train_labels.extend([tokenizer.decode(label, skip_special_tokens=True) for label in labels])\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, train Loss: {train_loss / len(train_loader):.4f}\")\n",
        "    # Compute train metrics\n",
        "    metrics = compute_metrics(train_predictions, train_labels)\n",
        "    print(f\"train Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n",
        "\n",
        "    print('test results')\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_predictions = []\n",
        "    val_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            val_loss += outputs.loss.item()\n",
        "            # Decode predictions and labels for metric calculation\n",
        "            predictions = model.generate(input_ids, attention_mask=attention_mask)\n",
        "            val_predictions.extend([tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions])\n",
        "            val_labels.extend([tokenizer.decode(label, skip_special_tokens=True) for label in labels])\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {val_loss / len(test_loader):.4f}\")\n",
        "    # Compute validation metrics\n",
        "    metrics = compute_metrics(val_predictions, val_labels)\n",
        "    print(f\"Validation Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5r3qjppz9Ge",
        "outputId": "4da42ee3-c644-4601-e947-6b81948ff25d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Epoch 1/1, Training Loss: 0.0002\n",
            "train results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, train Loss: 0.0001\n",
            "train Metrics: Accuracy: 0.9558, Precision: 0.9558, Recall: 0.9558, F1: 0.9558\n",
            "test results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, Validation Loss: 0.0004\n",
            "Validation Metrics: Accuracy: 0.9385, Precision: 0.9385, Recall: 0.9385, F1: 0.9385\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ok\n",
        "from transformers import BartForConditionalGeneration\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Training and validation loop\n",
        "epochs = 1\n",
        "\n",
        "def compute_metrics(preds, labels):\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    precision = precision_score(labels, preds, average='micro', zero_division=0)\n",
        "    recall = recall_score(labels, preds, average='micro', zero_division=0)\n",
        "    f1 = f1_score(labels, preds, average='micro', zero_division=0)\n",
        "    return accuracy, precision, recall, f1\n",
        "max_new_tokens=25\n",
        "for epoch in range(epochs):\n",
        "    # Training phase\n",
        "    print(epoch)\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    j=1\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        #print(j,\"  loss: \",loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        j=j+1\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {train_loss / len(train_loader):.4f}\")\n",
        "\n",
        "    print('train results')\n",
        "    model.eval()\n",
        "    train_loss = 0\n",
        "    train_predictions = []\n",
        "    train_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in train_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            train_loss += outputs.loss.item()\n",
        "\n",
        "            # Decode predictions and labels for metric calculation\n",
        "            predictions = model.generate(input_ids, attention_mask=attention_mask)\n",
        "            train_predictions.extend([tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions])\n",
        "            train_labels.extend([tokenizer.decode(label, skip_special_tokens=True) for label in labels])\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, train Loss: {train_loss / len(train_loader):.4f}\")\n",
        "    # Compute train metrics\n",
        "    metrics = compute_metrics(train_predictions, train_labels)\n",
        "    print(f\"train Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n",
        "\n",
        "    print('test results')\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_predictions = []\n",
        "    val_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            val_loss += outputs.loss.item()\n",
        "            # Decode predictions and labels for metric calculation\n",
        "            predictions = model.generate(input_ids, attention_mask=attention_mask)\n",
        "            val_predictions.extend([tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions])\n",
        "            val_labels.extend([tokenizer.decode(label, skip_special_tokens=True) for label in labels])\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {val_loss / len(test_loader):.4f}\")\n",
        "    # Compute validation metrics\n",
        "    metrics = compute_metrics(val_predictions, val_labels)\n",
        "    print(f\"Validation Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb30a1f2-19e6-4c4e-828a-04daec95efad",
        "id": "HMCz3xhGRDbu"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Epoch 1/1, Training Loss: 0.0002\n",
            "train results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, train Loss: 0.0000\n",
            "train Metrics: Accuracy: 0.9592, Precision: 0.9592, Recall: 0.9592, F1: 0.9592\n",
            "test results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, Validation Loss: 0.0003\n",
            "Validation Metrics: Accuracy: 0.9400, Precision: 0.9400, Recall: 0.9400, F1: 0.9400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#weighted results\n",
        "from transformers import BartForConditionalGeneration\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Training and validation loop\n",
        "epochs = 1\n",
        "\n",
        "def compute_metrics(preds, labels):\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    precision = precision_score(labels, preds, average='weighted', zero_division=0)\n",
        "    recall = recall_score(labels, preds, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(labels, preds, average='weighted', zero_division=0)\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "model.eval()\n",
        "train_loss = 0\n",
        "train_predictions = []\n",
        "train_labels = []\n",
        "with torch.no_grad():\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        train_loss += outputs.loss.item()\n",
        "\n",
        "        # Decode predictions and labels for metric calculation\n",
        "        predictions = model.generate(input_ids, attention_mask=attention_mask)\n",
        "        train_predictions.extend([tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions])\n",
        "        train_labels.extend([tokenizer.decode(label, skip_special_tokens=True) for label in labels])\n",
        "\n",
        "print(f\"Epoch {epoch + 1}/{epochs}, train Loss: {train_loss / len(train_loader):.4f}\")\n",
        "# Compute train metrics\n",
        "metrics = compute_metrics(train_predictions, train_labels)\n",
        "print(f\"train Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n",
        "\n",
        "print('test results')\n",
        "model.eval()\n",
        "val_loss = 0\n",
        "val_predictions = []\n",
        "val_labels = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        val_loss += outputs.loss.item()\n",
        "        # Decode predictions and labels for metric calculation\n",
        "        predictions = model.generate(input_ids, attention_mask=attention_mask)\n",
        "        val_predictions.extend([tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions])\n",
        "        val_labels.extend([tokenizer.decode(label, skip_special_tokens=True) for label in labels])\n",
        "\n",
        "print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {val_loss / len(test_loader):.4f}\")\n",
        "# Compute validation metrics\n",
        "metrics = compute_metrics(val_predictions, val_labels)\n",
        "print(f\"Validation Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bhCp4UGRC4W",
        "outputId": "f309b931-9578-4beb-884a-088575db4927"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, train Loss: 0.0000\n",
            "train Metrics: Accuracy: 0.9592, Precision: 0.9835, Recall: 0.9592, F1: 0.9667\n",
            "test results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, Validation Loss: 0.0003\n",
            "Validation Metrics: Accuracy: 0.9400, Precision: 0.9582, Recall: 0.9400, F1: 0.9459\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ok\n",
        "from transformers import BartForConditionalGeneration\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Training and validation loop\n",
        "epochs = 1\n",
        "\n",
        "def compute_metrics(preds, labels):\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    precision = precision_score(labels, preds, average='micro', zero_division=0)\n",
        "    recall = recall_score(labels, preds, average='micro', zero_division=0)\n",
        "    f1 = f1_score(labels, preds, average='micro', zero_division=0)\n",
        "    return accuracy, precision, recall, f1\n",
        "max_new_tokens=25\n",
        "for epoch in range(epochs):\n",
        "    # Training phase\n",
        "    print(epoch)\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    j=1\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        #print(j,\"  loss: \",loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        j=j+1\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {train_loss / len(train_loader):.4f}\")\n",
        "\n",
        "    print('train results')\n",
        "    model.eval()\n",
        "    train_loss = 0\n",
        "    train_predictions = []\n",
        "    train_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in train_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            train_loss += outputs.loss.item()\n",
        "\n",
        "            # Decode predictions and labels for metric calculation\n",
        "            predictions = model.generate(input_ids, attention_mask=attention_mask)\n",
        "            train_predictions.extend([tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions])\n",
        "            train_labels.extend([tokenizer.decode(label, skip_special_tokens=True) for label in labels])\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, train Loss: {train_loss / len(train_loader):.4f}\")\n",
        "    # Compute train metrics\n",
        "    metrics = compute_metrics(train_predictions, train_labels)\n",
        "    print(f\"train Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n",
        "\n",
        "    print('test results')\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_predictions = []\n",
        "    val_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            val_loss += outputs.loss.item()\n",
        "            # Decode predictions and labels for metric calculation\n",
        "            predictions = model.generate(input_ids, attention_mask=attention_mask)\n",
        "            val_predictions.extend([tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions])\n",
        "            val_labels.extend([tokenizer.decode(label, skip_special_tokens=True) for label in labels])\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {val_loss / len(test_loader):.4f}\")\n",
        "    # Compute validation metrics\n",
        "    metrics = compute_metrics(val_predictions, val_labels)\n",
        "    print(f\"Validation Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpnWo9GxXdev",
        "outputId": "79f8931e-9323-4366-d97c-16f578584f6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Epoch 1/1, Training Loss: 0.0002\n",
            "train results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, train Loss: 0.0000\n",
            "train Metrics: Accuracy: 0.9698, Precision: 0.9698, Recall: 0.9698, F1: 0.9698\n",
            "test results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, Validation Loss: 0.0003\n",
            "Validation Metrics: Accuracy: 0.9508, Precision: 0.9508, Recall: 0.9508, F1: 0.9508\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ok\n",
        "from transformers import BartForConditionalGeneration\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Training and validation loop\n",
        "epochs = 1\n",
        "\n",
        "def compute_metrics(preds, labels):\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    precision = precision_score(labels, preds, average='micro', zero_division=0)\n",
        "    recall = recall_score(labels, preds, average='micro', zero_division=0)\n",
        "    f1 = f1_score(labels, preds, average='micro', zero_division=0)\n",
        "    return accuracy, precision, recall, f1\n",
        "max_new_tokens=25\n",
        "for epoch in range(epochs):\n",
        "    # Training phase\n",
        "    print(epoch)\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    j=1\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        #print(j,\"  loss: \",loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        j=j+1\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {train_loss / len(train_loader):.4f}\")\n",
        "\n",
        "    print('train results')\n",
        "    model.eval()\n",
        "    train_loss = 0\n",
        "    train_predictions = []\n",
        "    train_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in train_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            train_loss += outputs.loss.item()\n",
        "\n",
        "            # Decode predictions and labels for metric calculation\n",
        "            predictions = model.generate(input_ids, attention_mask=attention_mask)\n",
        "            train_predictions.extend([tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions])\n",
        "            train_labels.extend([tokenizer.decode(label, skip_special_tokens=True) for label in labels])\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, train Loss: {train_loss / len(train_loader):.4f}\")\n",
        "    # Compute train metrics\n",
        "    metrics = compute_metrics(train_predictions, train_labels)\n",
        "    print(f\"train Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n",
        "\n",
        "    print('test results')\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_predictions = []\n",
        "    val_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            val_loss += outputs.loss.item()\n",
        "            # Decode predictions and labels for metric calculation\n",
        "            predictions = model.generate(input_ids, attention_mask=attention_mask)\n",
        "            val_predictions.extend([tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions])\n",
        "            val_labels.extend([tokenizer.decode(label, skip_special_tokens=True) for label in labels])\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {val_loss / len(test_loader):.4f}\")\n",
        "    # Compute validation metrics\n",
        "    metrics = compute_metrics(val_predictions, val_labels)\n",
        "    print(f\"Validation Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NigunBbfchWx",
        "outputId": "c5fbd97b-62e5-4882-d516-7614006af990"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Epoch 1/1, Training Loss: 0.0001\n",
            "train results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, train Loss: 0.0000\n",
            "train Metrics: Accuracy: 0.9735, Precision: 0.9735, Recall: 0.9735, F1: 0.9735\n",
            "test results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, Validation Loss: 0.0003\n",
            "Validation Metrics: Accuracy: 0.9600, Precision: 0.9600, Recall: 0.9600, F1: 0.9600\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#weighted results\n",
        "from transformers import BartForConditionalGeneration\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "epochs = 1\n",
        "\n",
        "def compute_metrics(preds, labels):\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    precision = precision_score(labels, preds, average='weighted', zero_division=0)\n",
        "    recall = recall_score(labels, preds, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(labels, preds, average='weighted', zero_division=0)\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "model.eval()\n",
        "train_loss = 0\n",
        "train_predictions = []\n",
        "train_labels = []\n",
        "with torch.no_grad():\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        train_loss += outputs.loss.item()\n",
        "\n",
        "        # Decode predictions and labels for metric calculation\n",
        "        predictions = model.generate(input_ids, attention_mask=attention_mask)\n",
        "        train_predictions.extend([tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions])\n",
        "        train_labels.extend([tokenizer.decode(label, skip_special_tokens=True) for label in labels])\n",
        "print(f\" train Loss: {train_loss / len(train_loader):.4f}\")\n",
        "# Compute train metrics\n",
        "metrics = compute_metrics(train_predictions, train_labels)\n",
        "print(f\"train Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n",
        "\n",
        "print('test results')\n",
        "model.eval()\n",
        "val_loss = 0\n",
        "val_predictions = []\n",
        "val_labels = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        val_loss += outputs.loss.item()\n",
        "        # Decode predictions and labels for metric calculation\n",
        "        predictions = model.generate(input_ids, attention_mask=attention_mask)\n",
        "        val_predictions.extend([tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions])\n",
        "        val_labels.extend([tokenizer.decode(label, skip_special_tokens=True) for label in labels])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBkKoPnLWXCV",
        "outputId": "d78b077d-002d-4a82-9673-e0e9818e905c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " train Loss: 0.0000\n",
            "train Metrics: Accuracy: 0.9735, Precision: 0.9867, Recall: 0.9735, F1: 0.9769\n",
            "test results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\" Validation Loss: {val_loss / len(test_loader):.4f}\")\n",
        "# Compute validation metrics\n",
        "metrics = compute_metrics(val_predictions, val_labels)\n",
        "print(f\"Validation Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F17GcrS7WlR7",
        "outputId": "16bd3ede-81aa-4186-9f03-3fcec027d569"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Validation Loss: 0.0003\n",
            "Validation Metrics: Accuracy: 0.9600, Precision: 0.9697, Recall: 0.9600, F1: 0.9616\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ok\n",
        "from transformers import BartForConditionalGeneration\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Training and validation loop\n",
        "epochs = 1\n",
        "\n",
        "def compute_metrics(preds, labels):\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    precision = precision_score(labels, preds, average='micro', zero_division=0)\n",
        "    recall = recall_score(labels, preds, average='micro', zero_division=0)\n",
        "    f1 = f1_score(labels, preds, average='micro', zero_division=0)\n",
        "    return accuracy, precision, recall, f1\n",
        "max_new_tokens=25\n",
        "for epoch in range(epochs):\n",
        "    # Training phase\n",
        "    print(epoch)\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    j=1\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        #print(j,\"  loss: \",loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        j=j+1\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {train_loss / len(train_loader):.4f}\")\n",
        "\n",
        "    print('train results')\n",
        "    model.eval()\n",
        "    train_loss = 0\n",
        "    train_predictions = []\n",
        "    train_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in train_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            train_loss += outputs.loss.item()\n",
        "\n",
        "            # Decode predictions and labels for metric calculation\n",
        "            predictions = model.generate(input_ids, attention_mask=attention_mask)\n",
        "            train_predictions.extend([tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions])\n",
        "            train_labels.extend([tokenizer.decode(label, skip_special_tokens=True) for label in labels])\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, train Loss: {train_loss / len(train_loader):.4f}\")\n",
        "    # Compute train metrics\n",
        "    metrics = compute_metrics(train_predictions, train_labels)\n",
        "    print(f\"train Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n",
        "\n",
        "    print('test results')\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_predictions = []\n",
        "    val_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            val_loss += outputs.loss.item()\n",
        "            # Decode predictions and labels for metric calculation\n",
        "            predictions = model.generate(input_ids, attention_mask=attention_mask)\n",
        "            val_predictions.extend([tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions])\n",
        "            val_labels.extend([tokenizer.decode(label, skip_special_tokens=True) for label in labels])\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {val_loss / len(test_loader):.4f}\")\n",
        "    # Compute validation metrics\n",
        "    metrics = compute_metrics(val_predictions, val_labels)\n",
        "    print(f\"Validation Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbdd12ce-f19c-4bce-f3b5-508707f2a963",
        "id": "kq7kEc_hovXV"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Epoch 1/1, Training Loss: 0.0001\n",
            "train results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, train Loss: 0.0000\n",
            "train Metrics: Accuracy: 0.9769, Precision: 0.9769, Recall: 0.9769, F1: 0.9769\n",
            "test results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, Validation Loss: 0.0004\n",
            "Validation Metrics: Accuracy: 0.9477, Precision: 0.9477, Recall: 0.9477, F1: 0.9477\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ok\n",
        "from transformers import BartForConditionalGeneration\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Training and validation loop\n",
        "epochs = 1\n",
        "\n",
        "def compute_metrics(preds, labels):\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    precision = precision_score(labels, preds, average='micro', zero_division=0)\n",
        "    recall = recall_score(labels, preds, average='micro', zero_division=0)\n",
        "    f1 = f1_score(labels, preds, average='micro', zero_division=0)\n",
        "    return accuracy, precision, recall, f1\n",
        "max_new_tokens=25\n",
        "for epoch in range(epochs):\n",
        "    # Training phase\n",
        "    print(epoch)\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    j=1\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        #print(j,\"  loss: \",loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        j=j+1\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {train_loss / len(train_loader):.4f}\")\n",
        "\n",
        "    print('train results')\n",
        "    model.eval()\n",
        "    train_loss = 0\n",
        "    train_predictions = []\n",
        "    train_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in train_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            train_loss += outputs.loss.item()\n",
        "\n",
        "            # Decode predictions and labels for metric calculation\n",
        "            predictions = model.generate(input_ids, attention_mask=attention_mask)\n",
        "            train_predictions.extend([tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions])\n",
        "            train_labels.extend([tokenizer.decode(label, skip_special_tokens=True) for label in labels])\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, train Loss: {train_loss / len(train_loader):.4f}\")\n",
        "    # Compute train metrics\n",
        "    metrics = compute_metrics(train_predictions, train_labels)\n",
        "    print(f\"train Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n",
        "\n",
        "    print('test results')\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_predictions = []\n",
        "    val_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            val_loss += outputs.loss.item()\n",
        "            # Decode predictions and labels for metric calculation\n",
        "            predictions = model.generate(input_ids, attention_mask=attention_mask)\n",
        "            val_predictions.extend([tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions])\n",
        "            val_labels.extend([tokenizer.decode(label, skip_special_tokens=True) for label in labels])\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {val_loss / len(test_loader):.4f}\")\n",
        "    # Compute validation metrics\n",
        "    metrics = compute_metrics(val_predictions, val_labels)\n",
        "    print(f\"Validation Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0588d0ad-c844-4c53-8a69-a42a9c001bae",
        "id": "DkLio9UkpxaF"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Epoch 1/1, Training Loss: 0.0001\n",
            "train results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, train Loss: 0.0000\n",
            "train Metrics: Accuracy: 0.9748, Precision: 0.9748, Recall: 0.9748, F1: 0.9748\n",
            "test results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, Validation Loss: 0.0003\n",
            "Validation Metrics: Accuracy: 0.9569, Precision: 0.9569, Recall: 0.9569, F1: 0.9569\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#weighted results\n",
        "from transformers import BartForConditionalGeneration\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Training and validation loop\n",
        "epochs = 1\n",
        "\n",
        "def compute_metrics(preds, labels):\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    precision = precision_score(labels, preds, average='weighted', zero_division=0)\n",
        "    recall = recall_score(labels, preds, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(labels, preds, average='weighted', zero_division=0)\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "model.eval()\n",
        "train_loss = 0\n",
        "train_predictions = []\n",
        "train_labels = []\n",
        "with torch.no_grad():\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        train_loss += outputs.loss.item()\n",
        "\n",
        "        # Decode predictions and labels for metric calculation\n",
        "        predictions = model.generate(input_ids, attention_mask=attention_mask)\n",
        "        train_predictions.extend([tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions])\n",
        "        train_labels.extend([tokenizer.decode(label, skip_special_tokens=True) for label in labels])\n",
        "\n",
        "print(f\"Epoch {epoch + 1}/{epochs}, train Loss: {train_loss / len(train_loader):.4f}\")\n",
        "# Compute train metrics\n",
        "metrics = compute_metrics(train_predictions, train_labels)\n",
        "print(f\"train Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n",
        "\n",
        "print('test results')\n",
        "model.eval()\n",
        "val_loss = 0\n",
        "val_predictions = []\n",
        "val_labels = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        val_loss += outputs.loss.item()\n",
        "        # Decode predictions and labels for metric calculation\n",
        "        predictions = model.generate(input_ids, attention_mask=attention_mask)\n",
        "        val_predictions.extend([tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions])\n",
        "        val_labels.extend([tokenizer.decode(label, skip_special_tokens=True) for label in labels])\n",
        "\n",
        "print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {val_loss / len(test_loader):.4f}\")\n",
        "# Compute validation metrics\n",
        "metrics = compute_metrics(val_predictions, val_labels)\n",
        "print(f\"Validation Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5af49d3f-f7b4-4a58-fd53-bd4cc4c05070",
        "id": "eMM7aOUach4z"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, train Loss: 0.0000\n",
            "train Metrics: Accuracy: 0.9748, Precision: 0.9890, Recall: 0.9748, F1: 0.9802\n",
            "test results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, Validation Loss: 0.0003\n",
            "Validation Metrics: Accuracy: 0.9569, Precision: 0.9743, Recall: 0.9569, F1: 0.9636\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  model.save_pretrained(\"/content/drive/MyDrive/Abreviation/bartf\")\n",
        "  tokenizer.save_pretrained(\"/content/drive/MyDrive/Abreviation/bartf\")"
      ],
      "metadata": {
        "id": "bA_IIjXS3kNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jvZQ6D3O8uzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XgJW29MUG1Ev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ok\n",
        "from transformers import BartForConditionalGeneration\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Training and validation loop\n",
        "epochs = 1\n",
        "\n",
        "def compute_metrics(preds, labels):\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    precision = precision_score(labels, preds, average='micro', zero_division=0)\n",
        "    recall = recall_score(labels, preds, average='micro', zero_division=0)\n",
        "    f1 = f1_score(labels, preds, average='micro', zero_division=0)\n",
        "    return accuracy, precision, recall, f1\n",
        "max_new_tokens=25\n",
        "for epoch in range(epochs):\n",
        "    # Training phase\n",
        "    print(epoch)\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    j=1\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        #print(j,\"  loss: \",loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        j=j+1\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {train_loss / len(train_loader):.4f}\")\n",
        "\n",
        "    print('train results')\n",
        "    model.eval()\n",
        "    train_loss = 0\n",
        "    train_predictions = []\n",
        "    train_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in train_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            train_loss += outputs.loss.item()\n",
        "\n",
        "            # Decode predictions and labels for metric calculation\n",
        "            predictions = model.generate(input_ids, attention_mask=attention_mask)\n",
        "            train_predictions.extend([tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions])\n",
        "            train_labels.extend([tokenizer.decode(label, skip_special_tokens=True) for label in labels])\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, train Loss: {train_loss / len(train_loader):.4f}\")\n",
        "    # Compute train metrics\n",
        "    metrics = compute_metrics(train_predictions, train_labels)\n",
        "    print(f\"train Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n",
        "\n",
        "    print('test results')\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_predictions = []\n",
        "    val_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            val_loss += outputs.loss.item()\n",
        "            # Decode predictions and labels for metric calculation\n",
        "            predictions = model.generate(input_ids, attention_mask=attention_mask)\n",
        "            val_predictions.extend([tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions])\n",
        "            val_labels.extend([tokenizer.decode(label, skip_special_tokens=True) for label in labels])\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {val_loss / len(test_loader):.4f}\")\n",
        "    # Compute validation metrics\n",
        "    metrics = compute_metrics(val_predictions, val_labels)\n",
        "    print(f\"Validation Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5a9c04f-1265-4701-c59d-4cccd33df600",
        "id": "NQVGhZJp8vXV"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Epoch 1/1, Training Loss: 0.0001\n",
            "train results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, train Loss: 0.0000\n",
            "train Metrics: Accuracy: 0.9727, Precision: 0.9727, Recall: 0.9727, F1: 0.9727\n",
            "test results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, Validation Loss: 0.0003\n",
            "Validation Metrics: Accuracy: 0.9531, Precision: 0.9531, Recall: 0.9531, F1: 0.9531\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ok\n",
        "from transformers import BartForConditionalGeneration\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Training and validation loop\n",
        "epochs = 1\n",
        "\n",
        "def compute_metrics(preds, labels):\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    precision = precision_score(labels, preds, average='micro', zero_division=0)\n",
        "    recall = recall_score(labels, preds, average='micro', zero_division=0)\n",
        "    f1 = f1_score(labels, preds, average='micro', zero_division=0)\n",
        "    return accuracy, precision, recall, f1\n",
        "max_new_tokens=25\n",
        "for epoch in range(epochs):\n",
        "    # Training phase\n",
        "    print(epoch)\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    j=1\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        #print(j,\"  loss: \",loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        j=j+1\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {train_loss / len(train_loader):.4f}\")\n",
        "\n",
        "    print('train results')\n",
        "    model.eval()\n",
        "    train_loss = 0\n",
        "    train_predictions = []\n",
        "    train_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in train_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            train_loss += outputs.loss.item()\n",
        "\n",
        "            # Decode predictions and labels for metric calculation\n",
        "            predictions = model.generate(input_ids, attention_mask=attention_mask)\n",
        "            train_predictions.extend([tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions])\n",
        "            train_labels.extend([tokenizer.decode(label, skip_special_tokens=True) for label in labels])\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, train Loss: {train_loss / len(train_loader):.4f}\")\n",
        "    # Compute train metrics\n",
        "    metrics = compute_metrics(train_predictions, train_labels)\n",
        "    print(f\"train Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n",
        "\n",
        "    print('test results')\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_predictions = []\n",
        "    val_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            val_loss += outputs.loss.item()\n",
        "            # Decode predictions and labels for metric calculation\n",
        "            predictions = model.generate(input_ids, attention_mask=attention_mask)\n",
        "            val_predictions.extend([tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions])\n",
        "            val_labels.extend([tokenizer.decode(label, skip_special_tokens=True) for label in labels])\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {val_loss / len(test_loader):.4f}\")\n",
        "    # Compute validation metrics\n",
        "    metrics = compute_metrics(val_predictions, val_labels)\n",
        "    print(f\"Validation Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n"
      ],
      "metadata": {
        "id": "2eb0s-G-XV9Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adb447d4-9eb2-4bcd-9e14-686028ce01a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Epoch 1/1, Training Loss: 0.0001\n",
            "train results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, train Loss: 0.0000\n",
            "train Metrics: Accuracy: 0.9871, Precision: 0.9871, Recall: 0.9871, F1: 0.9871\n",
            "test results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, Validation Loss: 0.0004\n",
            "Validation Metrics: Accuracy: 0.9608, Precision: 0.9608, Recall: 0.9608, F1: 0.9608\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ok\n",
        "from transformers import BartForConditionalGeneration\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Training and validation loop\n",
        "epochs = 1\n",
        "\n",
        "def compute_metrics(preds, labels):\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    precision = precision_score(labels, preds, average='micro', zero_division=0)\n",
        "    recall = recall_score(labels, preds, average='micro', zero_division=0)\n",
        "    f1 = f1_score(labels, preds, average='micro', zero_division=0)\n",
        "    return accuracy, precision, recall, f1\n",
        "max_new_tokens=25\n",
        "for epoch in range(epochs):\n",
        "    # Training phase\n",
        "    print(epoch)\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    j=1\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        #print(j,\"  loss: \",loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        j=j+1\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {train_loss / len(train_loader):.4f}\")\n",
        "\n",
        "    print('train results')\n",
        "    model.eval()\n",
        "    train_loss = 0\n",
        "    train_predictions = []\n",
        "    train_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in train_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            train_loss += outputs.loss.item()\n",
        "\n",
        "            # Decode predictions and labels for metric calculation\n",
        "            predictions = model.generate(input_ids, attention_mask=attention_mask)\n",
        "            train_predictions.extend([tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions])\n",
        "            train_labels.extend([tokenizer.decode(label, skip_special_tokens=True) for label in labels])\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, train Loss: {train_loss / len(train_loader):.4f}\")\n",
        "    # Compute train metrics\n",
        "    metrics = compute_metrics(train_predictions, train_labels)\n",
        "    print(f\"train Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n",
        "\n",
        "    print('test results')\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_predictions = []\n",
        "    val_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            val_loss += outputs.loss.item()\n",
        "            # Decode predictions and labels for metric calculation\n",
        "            predictions = model.generate(input_ids, attention_mask=attention_mask)\n",
        "            val_predictions.extend([tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions])\n",
        "            val_labels.extend([tokenizer.decode(label, skip_special_tokens=True) for label in labels])\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {val_loss / len(test_loader):.4f}\")\n",
        "    # Compute validation metrics\n",
        "    metrics = compute_metrics(val_predictions, val_labels)\n",
        "    print(f\"Validation Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFe0blN0Y0wA",
        "outputId": "2a0f103a-0301-4223-ebea-093b34e9c650"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Epoch 1/1, Training Loss: 0.0001\n",
            "train results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, train Loss: 0.0000\n",
            "train Metrics: Accuracy: 0.9890, Precision: 0.9890, Recall: 0.9890, F1: 0.9890\n",
            "test results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, Validation Loss: 0.0003\n",
            "Validation Metrics: Accuracy: 0.9654, Precision: 0.9654, Recall: 0.9654, F1: 0.9654\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uopaNimEiFoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#weighted results\n",
        "from transformers import BartForConditionalGeneration\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Training and validation loop\n",
        "epoch= 0\n",
        "\n",
        "def compute_metrics(preds, labels):\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    precision = precision_score(labels, preds, average='weighted', zero_division=0)\n",
        "    recall = recall_score(labels, preds, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(labels, preds, average='weighted', zero_division=0)\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "model.eval()\n",
        "train_loss = 0\n",
        "train_predictions = []\n",
        "train_labels = []\n",
        "with torch.no_grad():\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        train_loss += outputs.loss.item()\n",
        "\n",
        "        # Decode predictions and labels for metric calculation\n",
        "        predictions = model.generate(input_ids, attention_mask=attention_mask)\n",
        "        train_predictions.extend([tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions])\n",
        "        train_labels.extend([tokenizer.decode(label, skip_special_tokens=True) for label in labels])\n",
        "\n",
        "print(f\"Epoch {epoch + 1}/{epochs}, train Loss: {train_loss / len(train_loader):.4f}\")\n",
        "# Compute train metrics\n",
        "metrics = compute_metrics(train_predictions, train_labels)\n",
        "print(f\"train Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n",
        "\n",
        "print('test results')\n",
        "model.eval()\n",
        "val_loss = 0\n",
        "val_predictions = []\n",
        "val_labels = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        val_loss += outputs.loss.item()\n",
        "        # Decode predictions and labels for metric calculation\n",
        "        predictions = model.generate(input_ids, attention_mask=attention_mask)\n",
        "        val_predictions.extend([tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions])\n",
        "        val_labels.extend([tokenizer.decode(label, skip_special_tokens=True) for label in labels])\n",
        "\n",
        "print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {val_loss / len(test_loader):.4f}\")\n",
        "# Compute validation metrics\n",
        "metrics = compute_metrics(val_predictions, val_labels)\n",
        "print(f\"Validation Metrics: Accuracy: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, Recall: {metrics[2]:.4f}, F1: {metrics[3]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Glo0m2tfDuZ",
        "outputId": "4984812e-dcf0-4bc2-a75c-50dbece5bcb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, train Loss: 0.0000\n",
            "train Metrics: Accuracy: 0.9890, Precision: 0.9917, Recall: 0.9890, F1: 0.9893\n",
            "test results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, Validation Loss: 0.0003\n",
            "Validation Metrics: Accuracy: 0.9654, Precision: 0.9688, Recall: 0.9654, F1: 0.9640\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  model.save_pretrained(\"/content/drive/MyDrive/Abreviation/bartf\")\n",
        "  tokenizer.save_pretrained(\"/content/drive/MyDrive/Abreviation/bartf\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-DfrOtZiUZz",
        "outputId": "48bd6fc1-9684-4490-a007-c06291bf2b1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/Abreviation/bartf/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/Abreviation/bartf/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/Abreviation/bartf/vocab.json',\n",
              " '/content/drive/MyDrive/Abreviation/bartf/merges.txt',\n",
              " '/content/drive/MyDrive/Abreviation/bartf/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#End13_80"
      ],
      "metadata": {
        "id": "PD4DIVSCSlVX"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9082ecb6a42c43458d3ce14cfee77b70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cdb26fb2875f495c8ab6ead0b76041c8",
              "IPY_MODEL_2a45c40b886e4ae3b3606f7c6433a0c4",
              "IPY_MODEL_40bdc431282e4caba101c6269ba4e869"
            ],
            "layout": "IPY_MODEL_4ab30a7bc7dc4359b3ab0c700e8a35de"
          }
        },
        "cdb26fb2875f495c8ab6ead0b76041c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f01c7b00a72447fcade67154ca8a4b43",
            "placeholder": "",
            "style": "IPY_MODEL_f35911cdd19d4f01a29b91d7937493dc",
            "value": "vocab.json:100%"
          }
        },
        "2a45c40b886e4ae3b3606f7c6433a0c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7400f60d39a64897ad83859d5d1d0f0e",
            "max": 898823,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b725bfd735ac4bf8be68c1080f411858",
            "value": 898823
          }
        },
        "40bdc431282e4caba101c6269ba4e869": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf37322dcc234bb58cc199fc3054efdc",
            "placeholder": "",
            "style": "IPY_MODEL_f323b90f5e244688b4438b28e2e5fc23",
            "value": "899k/899k[00:00&lt;00:00,8.38MB/s]"
          }
        },
        "4ab30a7bc7dc4359b3ab0c700e8a35de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f01c7b00a72447fcade67154ca8a4b43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f35911cdd19d4f01a29b91d7937493dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7400f60d39a64897ad83859d5d1d0f0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b725bfd735ac4bf8be68c1080f411858": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cf37322dcc234bb58cc199fc3054efdc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f323b90f5e244688b4438b28e2e5fc23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3c1f7924b04c4ee8bf108a21997003ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b5ecfaab13ad443b89c25d1fd874c179",
              "IPY_MODEL_7db040cc57264ac884e86eef3a6089f0",
              "IPY_MODEL_cd8c54cc6bf2463294349f2731012182"
            ],
            "layout": "IPY_MODEL_8359effe92264b6f92ddcf6596fc3002"
          }
        },
        "b5ecfaab13ad443b89c25d1fd874c179": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe2c0e2a04664438bacde8d5142321e7",
            "placeholder": "",
            "style": "IPY_MODEL_f707e277ee444e269834a392040badbe",
            "value": "merges.txt:100%"
          }
        },
        "7db040cc57264ac884e86eef3a6089f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0697e0c9bfc84041827763dd3093cc10",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d8cfbbda45534c25ac163818725c7259",
            "value": 456318
          }
        },
        "cd8c54cc6bf2463294349f2731012182": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7558f64016654e2fbdec8367ebaa0c3d",
            "placeholder": "",
            "style": "IPY_MODEL_327d1d141660446180fc61a830a9e338",
            "value": "456k/456k[00:00&lt;00:00,723kB/s]"
          }
        },
        "8359effe92264b6f92ddcf6596fc3002": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe2c0e2a04664438bacde8d5142321e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f707e277ee444e269834a392040badbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0697e0c9bfc84041827763dd3093cc10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8cfbbda45534c25ac163818725c7259": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7558f64016654e2fbdec8367ebaa0c3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "327d1d141660446180fc61a830a9e338": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c40b12e08bd4829baf14923e5138670": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_578633dde09c4f699cf72b10e74f7dd9",
              "IPY_MODEL_41deb1bf40ee4b45aa339779f4df9c49",
              "IPY_MODEL_8da459e953df4cf180e4ed25c4bd6982"
            ],
            "layout": "IPY_MODEL_a47acd007293485d95365db0cc197bec"
          }
        },
        "578633dde09c4f699cf72b10e74f7dd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf776fd69e1640acb4b7d905034dd478",
            "placeholder": "",
            "style": "IPY_MODEL_d7871c4fbf174f1d9773d9d06ab2f5da",
            "value": "tokenizer.json:100%"
          }
        },
        "41deb1bf40ee4b45aa339779f4df9c49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c198d140e77e45baa30eefeaa15c1555",
            "max": 1355863,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ff16828b7e8d479d9c853c98c1c0ed68",
            "value": 1355863
          }
        },
        "8da459e953df4cf180e4ed25c4bd6982": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_409dd1e097be4706b3c935c43dfe3d99",
            "placeholder": "",
            "style": "IPY_MODEL_ef62a051dd0d414497f1dbfb476ed21f",
            "value": "1.36M/1.36M[00:00&lt;00:00,2.93MB/s]"
          }
        },
        "a47acd007293485d95365db0cc197bec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf776fd69e1640acb4b7d905034dd478": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7871c4fbf174f1d9773d9d06ab2f5da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c198d140e77e45baa30eefeaa15c1555": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff16828b7e8d479d9c853c98c1c0ed68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "409dd1e097be4706b3c935c43dfe3d99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef62a051dd0d414497f1dbfb476ed21f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c440caa9c7ea46958c8ae2063f3a04e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3a6b7fe39bca4b59a26383d82d62d0e3",
              "IPY_MODEL_a07c94c368ad4ae88e44c3f3779c2e72",
              "IPY_MODEL_4f0f86da0839470f845d38438e24169e"
            ],
            "layout": "IPY_MODEL_5421f5356b2e4266b574eb3fadd6eb97"
          }
        },
        "3a6b7fe39bca4b59a26383d82d62d0e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_695f3eb7f7d5402e87832556cf538f4a",
            "placeholder": "",
            "style": "IPY_MODEL_db8f6e71570940649eea413cec8e0305",
            "value": "config.json:100%"
          }
        },
        "a07c94c368ad4ae88e44c3f3779c2e72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8666c97565e04473be8f62496640449e",
            "max": 1716,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ba26dd6b0cfc43fd876602557c4a5f3d",
            "value": 1716
          }
        },
        "4f0f86da0839470f845d38438e24169e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e332312332b460481ce45a436a601fb",
            "placeholder": "",
            "style": "IPY_MODEL_5219b5750c2a4986a6f118d1ada44379",
            "value": "1.72k/1.72k[00:00&lt;00:00,70.6kB/s]"
          }
        },
        "5421f5356b2e4266b574eb3fadd6eb97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "695f3eb7f7d5402e87832556cf538f4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db8f6e71570940649eea413cec8e0305": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8666c97565e04473be8f62496640449e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba26dd6b0cfc43fd876602557c4a5f3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9e332312332b460481ce45a436a601fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5219b5750c2a4986a6f118d1ada44379": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "83d708d5e76744be882f377a955b7536": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e8b2208e7db74b4b9973c5dfa3c6b81e",
              "IPY_MODEL_3febdcdbe9e8464a850b0c88f63347c0",
              "IPY_MODEL_a5e434edf14e41f0b0fbf35b4a8ea232"
            ],
            "layout": "IPY_MODEL_14a10ed1328b4e19a697f5f5474e6d60"
          }
        },
        "e8b2208e7db74b4b9973c5dfa3c6b81e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7162a3bc22b45b0b07032fa7d6b246e",
            "placeholder": "",
            "style": "IPY_MODEL_3ffa79bc84ec43a284b0880ed435a4bb",
            "value": "model.safetensors:100%"
          }
        },
        "3febdcdbe9e8464a850b0c88f63347c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c36ed303b05403e9b5b9ef53b3f6594",
            "max": 557709915,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0bc289c4f7dc4d23aad174e31632cac2",
            "value": 557709915
          }
        },
        "a5e434edf14e41f0b0fbf35b4a8ea232": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8dde06a767c34369b6fcb0a388fbf62a",
            "placeholder": "",
            "style": "IPY_MODEL_ecd299eec28e4b1894921b2c3361eb4e",
            "value": "558M/558M[00:02&lt;00:00,229MB/s]"
          }
        },
        "14a10ed1328b4e19a697f5f5474e6d60": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7162a3bc22b45b0b07032fa7d6b246e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ffa79bc84ec43a284b0880ed435a4bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8c36ed303b05403e9b5b9ef53b3f6594": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0bc289c4f7dc4d23aad174e31632cac2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8dde06a767c34369b6fcb0a388fbf62a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ecd299eec28e4b1894921b2c3361eb4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "62f32d24e3d847c399d53fef502e8347": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4772f08870204f64b90aa60a6f8a08a4",
              "IPY_MODEL_854b11dea3af4b22bde63980a50a7d5e",
              "IPY_MODEL_f0202ff2438848a1876abeea7db35849"
            ],
            "layout": "IPY_MODEL_c917cbbdf96e43dba643c0922d672ecc"
          }
        },
        "4772f08870204f64b90aa60a6f8a08a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b751878b681422d95dc42010e58530f",
            "placeholder": "",
            "style": "IPY_MODEL_bad9f30feae54d95bb7dcff3560c4890",
            "value": "vocab.json:100%"
          }
        },
        "854b11dea3af4b22bde63980a50a7d5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d34852cd3ec74aba9f4252dd95229f79",
            "max": 898823,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_abfdef37e6524a0a8a88f05e3458b18b",
            "value": 898823
          }
        },
        "f0202ff2438848a1876abeea7db35849": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6cdc00d94be4457b8e184cc85905aac1",
            "placeholder": "",
            "style": "IPY_MODEL_98d85f91ec614212b44eb849bb9c48b2",
            "value": "899k/899k[00:00&lt;00:00,4.82MB/s]"
          }
        },
        "c917cbbdf96e43dba643c0922d672ecc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b751878b681422d95dc42010e58530f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bad9f30feae54d95bb7dcff3560c4890": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d34852cd3ec74aba9f4252dd95229f79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abfdef37e6524a0a8a88f05e3458b18b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6cdc00d94be4457b8e184cc85905aac1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98d85f91ec614212b44eb849bb9c48b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df692ebf26c24363a93c214fd964ce2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cdecb8541567431197dcd0f34b0ad8c2",
              "IPY_MODEL_700b61dc396a456a8c16093a55717d5d",
              "IPY_MODEL_93cb40852edf41399a763b42141d2d81"
            ],
            "layout": "IPY_MODEL_7a5302253e82448f98d43a7bae9c8793"
          }
        },
        "cdecb8541567431197dcd0f34b0ad8c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b4b64b5825c4f65bf180cc3c58f117a",
            "placeholder": "",
            "style": "IPY_MODEL_a4aeb8df54824d648a6d8391ff0853eb",
            "value": "merges.txt:100%"
          }
        },
        "700b61dc396a456a8c16093a55717d5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3936d27e7d546a986123b21debcb14d",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f5f512efd49a482ba07e485846004196",
            "value": 456318
          }
        },
        "93cb40852edf41399a763b42141d2d81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1460b7a91f9347cfa13e73f0c588ce02",
            "placeholder": "",
            "style": "IPY_MODEL_ec230163e0f24c39b5680d11cf5e1885",
            "value": "456k/456k[00:00&lt;00:00,3.53MB/s]"
          }
        },
        "7a5302253e82448f98d43a7bae9c8793": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b4b64b5825c4f65bf180cc3c58f117a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4aeb8df54824d648a6d8391ff0853eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e3936d27e7d546a986123b21debcb14d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5f512efd49a482ba07e485846004196": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1460b7a91f9347cfa13e73f0c588ce02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec230163e0f24c39b5680d11cf5e1885": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "111b5f52abac48bfa4b67af55d5fcb73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_175029bb4f8145e3bfe1f1e965971246",
              "IPY_MODEL_19056a00e2fc4626982ef99697718c59",
              "IPY_MODEL_3e399096383f46d9816a9423fb4b63ca"
            ],
            "layout": "IPY_MODEL_359905773a9740778f3eb0f2a76b37b7"
          }
        },
        "175029bb4f8145e3bfe1f1e965971246": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05a63f61e6b54fbb8e67eb6da2d6fb2d",
            "placeholder": "",
            "style": "IPY_MODEL_448952a091b243acb267809b6c434c4e",
            "value": "tokenizer.json:100%"
          }
        },
        "19056a00e2fc4626982ef99697718c59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc3998a0654e4651b15b5e0de653df95",
            "max": 1355863,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e36d1b1253d349059437046492664e6f",
            "value": 1355863
          }
        },
        "3e399096383f46d9816a9423fb4b63ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56391f1b3651403eb2f770092c4c8d9b",
            "placeholder": "",
            "style": "IPY_MODEL_2d3e20b84e7d48baacc395efb4991d35",
            "value": "1.36M/1.36M[00:00&lt;00:00,5.22MB/s]"
          }
        },
        "359905773a9740778f3eb0f2a76b37b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05a63f61e6b54fbb8e67eb6da2d6fb2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "448952a091b243acb267809b6c434c4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fc3998a0654e4651b15b5e0de653df95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e36d1b1253d349059437046492664e6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "56391f1b3651403eb2f770092c4c8d9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d3e20b84e7d48baacc395efb4991d35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "acc4526414d6486083b26a1ba8e5da20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_343561b31a4c44d391810591fce60a61",
              "IPY_MODEL_61c89ea7b98c499f96ea6f15a4e6927a",
              "IPY_MODEL_89f55a4462d04cc9be17b91d6e09a760"
            ],
            "layout": "IPY_MODEL_96bd31ada74e4b399245691fc9c247f7"
          }
        },
        "343561b31a4c44d391810591fce60a61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_442b634729324a7bba7ed4c0833d4027",
            "placeholder": "",
            "style": "IPY_MODEL_4845bdb13f42433189b38147f9f6c6d5",
            "value": "config.json:100%"
          }
        },
        "61c89ea7b98c499f96ea6f15a4e6927a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6da71279c1a2460eb7b2b08789119a2d",
            "max": 1716,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cea6f0a34e0e436090a2be7cbd1408c5",
            "value": 1716
          }
        },
        "89f55a4462d04cc9be17b91d6e09a760": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d472d11df7d04923bd57468ddba3e73f",
            "placeholder": "",
            "style": "IPY_MODEL_8004a1892ae94e9f9ec281ed728c2abe",
            "value": "1.72k/1.72k[00:00&lt;00:00,52.8kB/s]"
          }
        },
        "96bd31ada74e4b399245691fc9c247f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "442b634729324a7bba7ed4c0833d4027": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4845bdb13f42433189b38147f9f6c6d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6da71279c1a2460eb7b2b08789119a2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cea6f0a34e0e436090a2be7cbd1408c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d472d11df7d04923bd57468ddba3e73f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8004a1892ae94e9f9ec281ed728c2abe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}